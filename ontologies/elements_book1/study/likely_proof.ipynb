{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eaf3154",
   "metadata": {},
   "source": [
    "# Likely activation potential: SPEC\n",
    "\n",
    "This notebook will implement the *likely co-occurrence* component and likely activation potential from `main.tex`, using the same ontology input and operational choices as `typical_proof.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897920ba",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Compute likely activation potential for resources used in each proof `N`, using:\n",
    "- context `C` (same as in `typical_proof.ipynb`),\n",
    "- a *salient set* `S` defined by one of two variants (selectable by a flag),\n",
    "- likely co-occurrence `Φ_{L_c}(r, C, S)` and likely activation `Φ_L(r, C, S)`.\n",
    "\n",
    "Output one CSV per analysis; filenames must encode key parameters and include a timestamp.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581008ce",
   "metadata": {},
   "source": [
    "## Inputs / Parameters\n",
    "- `START_PROPOSITION`, `END_PROPOSITION`: proof range (same semantics as `typical_proof.ipynb`).\n",
    "- `EPSILON`: weight for `Φ_L = ε * Φ_h + (1 - ε) * Φ_{L_c}` (parallel to `DELTA` in typical).\n",
    "- `HISTORY_WEIGHTS`: 3-tuple `(α, β, γ)` for direct/hierarchical/mereological histories (same validation rules).\n",
    "- `TYPE_SELECTION`: boolean, if `True` use relation/operation types in proposition/proof queries; if `False` use direct concepts.\n",
    "- `S_VARIANT`: enum flag in `{\"statement_only\", \"statement_plus_related_chunks\"}` selecting the salient set definition.\n",
    "- `EXCLUDED_CONCEPT_IRIS`, `EXCLUDED_CONCEPT_IRI_SUBSTRINGS`: same filtering behavior as in `typical_proof.ipynb`.\n",
    "- Input TTL: use the same selection logic as `typical_proof.ipynb` (latest TTL in `ontologies/`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7bf0e1",
   "metadata": {},
   "source": [
    "## Context `C` (same as `typical_proof.ipynb`)\n",
    "For each proof `N`:\n",
    "- `C` includes resources from definitions, postulates, common notions, and propositions up to `N` (included), plus proofs up to `N-1` (included).\n",
    "- Use the same query families as `typical_proof.ipynb` for direct / hierarchical / mereological histories and Hebbian co-occurrence.\n",
    "- Apply the same exclusion filters after each query materializes.\n",
    "- Build `hebb_C` using the same operational definition of “together” as `typical_proof.ipynb`:\n",
    "  resources co-occur if they are used in the same definition, postulate, common notion, proposition, or proof (via `refers_to` / `contains_concept`).\n",
    "- Preserve the same ordered-pair caveat used in `typical_proof.ipynb` (queries return `(o1, o2)` pairs).\n",
    "- Counting convention: keep duplicates (multiset semantics). When combining query outputs, do not deduplicate by resource; retain multiplicities/counts and aggregate by summing counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1631c13e",
   "metadata": {},
   "source": [
    "## Salient set `S` (two variants)\n",
    "Let `last_proposition` be proposition `N` (the statement immediately preceding proof `N`).\n",
    "Assumption: proof `N` is always immediately after proposition `N`.\n",
    "\n",
    "**Variant 1: `S_VARIANT = \"statement_only\"`**\n",
    "- `S` = resources in the *statement* of `last_proposition`.\n",
    "- Implement via a dedicated SPARQL query that extracts resources from the statement, with `TYPE_SELECTION` applied.\n",
    "- What \"resources from the statement\" means will be operationalized by the queries below. It does _not_ mean that the resources ought to occur in the statement _directly_.\n",
    "\n",
    "**Variant 2: `S_VARIANT = \"statement_plus_related_chunks\"`**\n",
    "- Let `S0` be defined operationally as follows: collect statement-resource candidates from the statement-only extraction queries for `last_proposition`, apply `EXCLUDED_CONCEPT_IRIS` and `EXCLUDED_CONCEPT_IRI_SUBSTRINGS`, then deduplicate by IRI. In symbols: `S0 := dedup(filter(candidates_from_statement(last_proposition)))`.\n",
    "- Let `R` be the set of resources that occur in any chunk (definition, postulate, common notion, proposition, or proof) before proposition N that shares at least one resource with `S0`. The definition of \"occurs in\" will be operationalized below and it may differ in different cases.\n",
    "- Then `S = S0 ∪ R`.\n",
    "- Implement via a dedicated SPARQL query that\n",
    "  (a) identifies chunks sharing at least one resource with `S0`, and\n",
    "  (b) returns all resources from those chunks, with `TYPE_SELECTION` applied.\n",
    "\n",
    "For each variant, apply the same exclusion filters as in `typical_proof.ipynb`.\n",
    "\n",
    "The multiset ‘keep duplicates’ rule applies to context/history/hebb aggregations; S is always deduplicated (set semantics)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b669c",
   "metadata": {},
   "source": [
    "## Likely co-occurrence (`Φ_{L_c}`)\n",
    "- Build `hebb_C` from query outputs as ordered pairs `(o1, o2, links)`, aggregated by sum of `links` per ordered pair.\n",
    "- Define `deg_C^S(r) = sum_{s in S, s != r} (hebb_C(r, s) + hebb_C(s, r))` (symmetrize at use time).\n",
    "- Define denominator over unique context resources: let `U_C` be the set of unique resource IRIs in context `C` after filtering; then `Z = sum_{u in U_C} deg_C^S(u)`.\n",
    "- Set `Φ_{L_c}(r, C, S) = deg_C^S(r) / Z` if `Z > 0`, else `0`.\n",
    "- This is computed only for resources *used in proof `N`* (same as typical).\n",
    "\n",
    "## Likely activation potential (`Φ_L`)\n",
    "- Compute historical component `Φ_h(r, C)` using the same direct/hierarchical/mereological histories and weights as typical.\n",
    "- Combine with likely co-occurrence:\n",
    "  `Φ_L(r, C, S) = ε * Φ_h(r, C) + (1 - ε) * Φ_{L_c}(r, C, S)`\n",
    "  where `ε = EPSILON` in `[0, 1]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6949e215",
   "metadata": {},
   "source": [
    "## Four analyses (S variant × TYPE_SELECTION)\n",
    "Run all four combinations:\n",
    "1. `S_VARIANT=statement_only`, `TYPE_SELECTION=False`\n",
    "2. `S_VARIANT=statement_only`, `TYPE_SELECTION=True`\n",
    "3. `S_VARIANT=statement_plus_related_chunks`, `TYPE_SELECTION=False`\n",
    "4. `S_VARIANT=statement_plus_related_chunks`, `TYPE_SELECTION=True`\n",
    "\n",
    "Execution mode note: this notebook is organized into separate sections so each combination can be run independently.\n",
    "You can pick and choose which combinations to execute; the list above is a coverage checklist, not a single mandatory run.\n",
    "\n",
    "Each run should iterate proofs `N` in `[START_PROPOSITION, END_PROPOSITION]`, compute\n",
    "`Φ_h`, `Φ_{L_c}`, `Φ_L`, and the set of new resources (same definition as in typical).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c8bea",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "For each analysis, write one CSV with rows per `(proof, resource)` and mirror the structure of `typical_proof.ipynb`:\n",
    "- `proof`\n",
    "- `resource_used_in_proof`\n",
    "- `number_of_resources_used_in_proof`\n",
    "- `phi_h`\n",
    "- `phi_lc` (likely co-occurrence)\n",
    "- `phi_l` (likely activation)\n",
    "- `new_resources`\n",
    "- `number_of_new_resources`\n",
    "- `new_resources` and `number_of_new_resources` are per-proof data and are repeated across all rows of the same proof (as in `typical_proof.ipynb`).\n",
    "\n",
    "**Filename requirements**\n",
    "- Must include: `S_VARIANT`, `TYPE_SELECTION`, proof range, EPSILON, HISTORY_WEIGHTS, and a timestamp.\n",
    "- Example pattern: `likely_{s_variant}_type_{type_flag}_eps-{epsilon}_w-{history_weights}_p{start}-{end}_{YYYYmmdd_HHMMSS}.csv`\n",
    "\n",
    "The output directory should mirror `typical_proof.ipynb` (e.g., `output/`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dcf032",
   "metadata": {},
   "source": [
    "## SPARQL queries\n",
    "\n",
    "\n",
    "CASE 1: salient_statement_resources\n",
    "For salient_statement_resources(last_proposition, type_selection=False) use the following SPARQL queries:\n",
    "- queries.direct_template_propositions_proofs(last_proposition)\n",
    "- queries.hierarchical_template_propositions_proofs(last_proposition) [super-concepts of statement resources are statement resources]\n",
    "- queries.mereological_template_propositions_proofs(last_proposition). [components of statement resources are statement resources]\n",
    "\n",
    "For salient_statement_resources(last_proposition, type_selection=True) use the following SPARQL queries:\n",
    "- queries.direct_template_last_item_types(last_proposition)\n",
    "- queries.hierarchical_template_propositions_proofs(last_proposition) [super-concepts of statement resources are statement resources]\n",
    "- queries.mereological_template_propositions_proofs(last_proposition). [components of statement resources are statement resources]\n",
    "\n",
    "Clarification: for `TYPE_SELECTION=True`, this mixed setup is intentional and correct: direct statement resources are taken at type level, while hierarchical and mereological expansions remain concept-level as listed.\n",
    "\n",
    "These SPARQL queries provide resources and counts. Then the notebook can use these results to proceed with the required calculations.\n",
    "\n",
    "CASE 2: salient_statement_plus_related_chunks \n",
    "For salient_statement_plus_related_chunks(last_proposition, type_selection=False) use the following SPARQL queries:\n",
    "(a) queries.direct_template_propositions_proofs(last_proposition)\n",
    "(b) queries.hierarchical_template_propositions_proofs(last_proposition) [super-concepts of statement resources are statement resources]\n",
    "(c) queries.mereological_template_propositions_proofs(last_proposition) [components of statement resources are statement resources]\n",
    "(d) queries.find_salient_definitions_postulates_common_notions(resource_iris) [outputs: IRIs of definitions, postulates, common notions]\n",
    "Note on query (e): in `queries.find_salient_propositions_proofs(resource_iris, proposition)`, `proposition` must be a proposition IRI string (e.g., `<https://www.foom.com/core#proposition_17>`), not a numeric index. The query returns propositions/proofs strictly before that proposition.\n",
    "(e) queries.find_salient_propositions_proofs(resource_iris, proposition) [outputs: IRIs of propositions, and proofs]\n",
    "(f) queries.direct_definitions_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(g) queries.direct_postulates_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(h) queries.direct_common_notions_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(i) queries.hierarchical_definitions_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(j) queries.hierarchical_postulates_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(k) queries.hierarchical_common_notions_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(l) queries.mereological_definitions_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(m) queries.mereological_postulates_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(n) queries.mereological_common_notions_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(o) queries.direct_template_propositions_proofs_selected_values(iri_of_salient_resources). [use IRIs from queries (e)]\n",
    "Contract for (d): it musts return the IRIs to be used as `iri_of_salient_resources` VALUES input for queries (f) through (n). \n",
    "Contract for (e): it musts return the IRIs to be used as `iri_of_salient_resources` VALUES input for query (o).\n",
    "\n",
    "For salient_statement_plus_related_chunks(last_proposition, type_selection=True) use the following SPARQL queries:\n",
    "(a) queries.direct_template_last_item_types(last_proposition)\n",
    "(b) queries.hierarchical_template_propositions_proofs(last_proposition) [super-concepts of statement resources are statement resources]\n",
    "(c) queries.mereological_template_propositions_proofs(last_proposition) [components of statement resources are statement resources]\n",
    "(d) queries.find_salient_definitions_postulates_common_notions(resource_iris)  [outputs: IRIs of definitions, postulates, common notions]\n",
    "(e) queries.find_salient_propositions_proofs(resource_iris, proposition) [outputs: IRIs of propositions, and proofs]\n",
    "(f) queries.direct_definitions_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(g) queries.direct_postulates_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(h) queries.direct_common_notions_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(i) queries.hierarchical_definitions_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(j) queries.hierarchical_postulates_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(k) queries.hierarchical_common_notions_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(l) queries.mereological_definitions_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(m) queries.mereological_postulates_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(n) queries.mereological_common_notions_selected_values(iri_of_salient_resources) [use IRIs from queries (d)]\n",
    "(o) queries.direct_template_last_item_types(iri_of_salient_resources). [use IRIs from queries (e)]\n",
    "Contract for (d): it must return the IRIs to be used as `iri_of_salient_resources` VALUES input for queries (f) through (n). \n",
    "Contract for (e): it must return the IRIs to be used as `iri_of_salient_resources` VALUES input for query (o).\n",
    "\n",
    "The queries (a), (b), and (c) find both the resources that are salient in the last proposition and the counts. \n",
    "Use resources from (a)-(c) only to construct `resource_iris` for queries (d) and (e) after filtering and deduplication.\n",
    "Use counts from (f) through (o) only to build chunk/resource multiplicities for downstream context/history/hebb aggregations; do not use any salient-extraction counts as weights in `S`, `deg_C^S`, or `\\Phi_{L_c}`.\n",
    "\n",
    "TYPE_SELECTION=True can narrow chunk retrieval in vairant 2: this is an accepted consequence. This is intentional.\n",
    "TYPE_SELECTION is applied correctly in Variant 2: only some queries are affected by TYPE_SELECTION.\n",
    "\n",
    "The spec requires that all queries:\n",
    "- are filtered by `EXCLUDED_CONCEPT_IRIS` and `EXCLUDED_CONCEPT_IRI_SUBSTRINGS`\n",
    "  immediately after each query result is materialized.\n",
    "\n",
    "NOTE: main.tex intentionally leaves “used in C” and “used together” open to different\n",
    "  operationalizations. So the asymmetry in likely_proof.ipynb is allowed and does not\n",
    "  contradict the theory; it just needs to be stated explicitly as a modeling choice.\n",
    "\n",
    "\n",
    "\n",
    "Note on salient-query counts vs set `S`\n",
    "In this notebook, `S` is a **set** (membership-only), not a weighted set.\n",
    "\n",
    "- Counts returned by salient-set extraction queries `(a)(b)(c)` are used only to gather/aggregate candidate resources before filtering and deduplication.\n",
    "- After exclusions, candidates are deduplicated by IRI to form `S`.\n",
    "- These counts do **not** weight `deg_C^S` or `\\Phi_{L_c}`.\n",
    "\n",
    "Computation remains:\n",
    "\\[\n",
    "\\deg_C^S(r)=\\sum_{s\\in S,\\ s\\neq r}\\big(hebb_C(r,s)+hebb_C(s,r)\\big),\\quad\n",
    "\\Phi_{L_c}(r,C,S)=\\frac{\\deg_C^S(r)}{\\sum_{u\\in U_C}\\deg_C^S(u)}\n",
    "\\]\n",
    "(with `0` when the denominator is `0`; here `U_C` is the set of unique context resource IRIs after filtering).\n",
    "\n",
    "If weighted salience is desired, that is a different model and requires explicit formula changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28231579",
   "metadata": {},
   "source": [
    "## Validation / Edge cases\n",
    "- S is a set and may be empty after filtering.\n",
    "If S = ∅, define deg_C^S(r) = 0 for all resources r, so Φ_{L_c}(r,C,S) = 0 for all r.\n",
    "Therefore Φ_L(r,C,S) = EPSILON * Φ_h(r,C).\n",
    "\n",
    "Operational rule:\n",
    "\n",
    "If salient-resource extraction yields no resources, skip salient-dependent SPARQL steps (find_salient_*, *_selected_values) and treat their outputs as empty.\n",
    "Continue the proof iteration normally and still write output rows.\n",
    "\n",
    "- If `hebb_C` is empty, `Φ_{L_c}` must be 0.\n",
    "\n",
    "- Ensure `EPSILON ∈ [0,1]` and `HISTORY_WEIGHTS` sum to 1.\n",
    "\n",
    "- For `N=1`, context contains definitions, postulates, common notions, and proposition 1; there are no prior proofs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72a397",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a601a68",
   "metadata": {},
   "source": [
    "# Likely activation potential: IMPLEMENTATION\n",
    "\n",
    "Focused implementation of the approved spec using minimal new modules and the existing typical pipeline helpers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from modules import file_utils, rdf_utils\n",
    "from modules.exclusion_filters import normalize_excluded_iris\n",
    "from modules.likely_activation import compute_phi_l, compute_phi_lc\n",
    "from modules.likely_context import build_context_for_proof\n",
    "from modules.likely_salience import S_VARIANTS, build_salient_set_for_proof\n",
    "from modules.query_runner import QueryRunner\n",
    "from modules.typical_activation import compute_new_resources, compute_phi_h\n",
    "from modules.typical_proof_resources import resources_in_proof\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457eda63",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path('output')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INPUT_TTL = file_utils.latest_file(folder=Path('ontologies'), filename_fragment='ontology_', extension='ttl')\n",
    "graph = rdf_utils.load_graph(INPUT_TTL)\n",
    "runner = QueryRunner(graph)\n",
    "\n",
    "# Parameters\n",
    "EPSILON = 1/2\n",
    "HISTORY_WEIGHTS = (6 / 9, 1 / 9, 2 / 9)\n",
    "START_PROPOSITION = 1\n",
    "END_PROPOSITION = 48\n",
    "\n",
    "EXCLUDED_CONCEPT_IRIS = [\n",
    "    \"https://www.foom.com/core#concept__one\",\n",
    "    \"https://www.foom.com/core#concept__two\",\n",
    "    \"https://www.foom.com/core#concept__three\",\n",
    "    \"https://www.foom.com/core#concept__four\",\n",
    "    \"https://www.foom.com/core#concept__are_same\",\n",
    "    \"https://www.foom.com/core#concept__possible\",\n",
    "    \"https://www.foom.com/core#concept__is_half\",\n",
    "    \"https://www.foom.com/core#concept__are_same\",\n",
    "    \"https://www.foom.com/core#concept___thing\",\n",
    "]\n",
    "\n",
    "EXCLUDED_CONCEPT_IRI_SUBSTRINGS = [\n",
    "    \"https://www.foom.com/core#set_\",\n",
    "]\n",
    "\n",
    "def validate_params() -> None:\n",
    "    if not (0.0 <= EPSILON <= 1.0):\n",
    "        raise ValueError(f\"EPSILON must be in [0, 1], got {EPSILON}.\")\n",
    "    if len(HISTORY_WEIGHTS) != 3:\n",
    "        raise ValueError(f\"HISTORY_WEIGHTS must have length 3, got {len(HISTORY_WEIGHTS)}.\")\n",
    "    if any((w < 0.0 or w > 1.0) for w in HISTORY_WEIGHTS):\n",
    "        raise ValueError(\"All HISTORY_WEIGHTS must be in [0, 1].\")\n",
    "    total = sum(HISTORY_WEIGHTS)\n",
    "    if abs(total - 1.0) > 1e-9:\n",
    "        raise ValueError(f\"HISTORY_WEIGHTS must sum to 1, got {total}.\")\n",
    "    if START_PROPOSITION > END_PROPOSITION:\n",
    "        raise ValueError(\"START_PROPOSITION must be <= END_PROPOSITION.\")\n",
    "    if any(not isinstance(value, str) for value in EXCLUDED_CONCEPT_IRIS):\n",
    "        raise ValueError(\"EXCLUDED_CONCEPT_IRIS must contain only strings.\")\n",
    "    if any(not isinstance(value, str) for value in EXCLUDED_CONCEPT_IRI_SUBSTRINGS):\n",
    "        raise ValueError(\"EXCLUDED_CONCEPT_IRI_SUBSTRINGS must contain only strings.\")\n",
    "\n",
    "validate_params()\n",
    "EXCLUDED_IRIS = normalize_excluded_iris(EXCLUDED_CONCEPT_IRIS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(s_variant: str, type_selection: bool) -> tuple[pd.DataFrame, Path]:\n",
    "    if s_variant not in S_VARIANTS:\n",
    "        raise ValueError(f\"Unknown s_variant {s_variant}. Expected one of {sorted(S_VARIANTS)}\")\n",
    "\n",
    "    rows: list[dict[str, object]] = []\n",
    "\n",
    "    for proof_n in range(START_PROPOSITION, END_PROPOSITION + 1):\n",
    "        context_resources, family_dfs, hebb_df = build_context_for_proof(\n",
    "            proof_n,\n",
    "            runner=runner,\n",
    "            type_selection=type_selection,\n",
    "            excluded_iris=EXCLUDED_IRIS,\n",
    "            excluded_substrings=EXCLUDED_CONCEPT_IRI_SUBSTRINGS,\n",
    "        )\n",
    "        proof_resources = resources_in_proof(\n",
    "            proof_n,\n",
    "            runner=runner,\n",
    "            type_selection=type_selection,\n",
    "            excluded_iris=EXCLUDED_IRIS,\n",
    "            excluded_substrings=EXCLUDED_CONCEPT_IRI_SUBSTRINGS,\n",
    "        )\n",
    "        proof_resources_sorted = sorted(proof_resources)\n",
    "\n",
    "        salient_set = build_salient_set_for_proof(\n",
    "            proof_n,\n",
    "            runner=runner,\n",
    "            type_selection=type_selection,\n",
    "            s_variant=s_variant,\n",
    "            excluded_iris=EXCLUDED_IRIS,\n",
    "            excluded_substrings=EXCLUDED_CONCEPT_IRI_SUBSTRINGS,\n",
    "        )\n",
    "\n",
    "        phi_h_df = compute_phi_h(proof_resources_sorted, family_dfs, HISTORY_WEIGHTS)\n",
    "        phi_lc_df = compute_phi_lc(\n",
    "            proof_resources_sorted,\n",
    "            hebb_df,\n",
    "            salient_set,\n",
    "            context_resources,\n",
    "        )\n",
    "        phi_l_df = compute_phi_l(proof_resources_sorted, phi_h_df, phi_lc_df, EPSILON)\n",
    "\n",
    "        new_resources = compute_new_resources(proof_resources_sorted, context_resources)\n",
    "        proof_count = len(proof_resources_sorted)\n",
    "        new_count = len(new_resources)\n",
    "\n",
    "        phi_h_map = dict(zip(phi_h_df[\"resource_used_in_proof\"], phi_h_df[\"phi_h\"]))\n",
    "        phi_lc_map = dict(zip(phi_lc_df[\"resource_used_in_proof\"], phi_lc_df[\"phi_lc\"]))\n",
    "        phi_l_map = dict(zip(phi_l_df[\"resource_used_in_proof\"], phi_l_df[\"phi_l\"]))\n",
    "\n",
    "        for resource in proof_resources_sorted:\n",
    "            rows.append({\n",
    "                \"proof\": proof_n,\n",
    "                \"resource_used_in_proof\": resource,\n",
    "                \"number_of_resources_used_in_proof\": proof_count,\n",
    "                \"phi_h\": float(phi_h_map.get(resource, 0.0)),\n",
    "                \"phi_lc\": float(phi_lc_map.get(resource, 0.0)),\n",
    "                \"phi_l\": float(phi_l_map.get(resource, 0.0)),\n",
    "                \"new_resources\": new_resources,\n",
    "                \"number_of_new_resources\": new_count,\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"proof\",\n",
    "            \"resource_used_in_proof\",\n",
    "            \"number_of_resources_used_in_proof\",\n",
    "            \"phi_h\",\n",
    "            \"phi_lc\",\n",
    "            \"phi_l\",\n",
    "            \"new_resources\",\n",
    "            \"number_of_new_resources\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    history_weights_label = \"-\".join(f\"{w:.4f}\" for w in HISTORY_WEIGHTS)\n",
    "    epsilon_label = f\"{EPSILON:.4f}\"\n",
    "    timestamp = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    type_label = str(type_selection).lower()\n",
    "    output_path = OUTPUT_DIR / (\n",
    "        f\"likely_{s_variant}_type-{type_label}_eps-{epsilon_label}_w-{history_weights_label}_\"\n",
    "        f\"p{START_PROPOSITION}-{END_PROPOSITION}_{timestamp}.csv\"\n",
    "    )\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"[{s_variant} | type={type_label}] proofs={END_PROPOSITION - START_PROPOSITION + 1}, rows={len(results_df)}\")\n",
    "    print(f\"Saved: {output_path}\")\n",
    "    return results_df, output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce26834",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ALL_COMBINATIONS = True\n",
    "SELECTED_RUNS = [\n",
    "    (\"statement_only\", False),\n",
    "    (\"statement_only\", True),\n",
    "    (\"statement_plus_related_chunks\", False),\n",
    "    (\"statement_plus_related_chunks\", True),\n",
    "]\n",
    "\n",
    "runs = SELECTED_RUNS if RUN_ALL_COMBINATIONS else SELECTED_RUNS[:1]\n",
    "all_results: dict[tuple[str, bool], tuple[pd.DataFrame, Path]] = {}\n",
    "for s_variant, type_selection in runs:\n",
    "    all_results[(s_variant, type_selection)] = run_analysis(s_variant, type_selection)\n",
    "\n",
    "all_results\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
