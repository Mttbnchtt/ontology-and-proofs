{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Study Notebook\n",
    "\n",
    "to-do: use RDFox instead of rdflib to account for implications (and, in particular, owl:sameAs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "NOTEBOOK_DIR = pathlib.Path.cwd()\n",
    "if '_NB_SYS_PATH_ADJUSTED' not in globals():\n",
    "    sys.path.insert(0, str(NOTEBOOK_DIR))\n",
    "    _NB_SYS_PATH_ADJUSTED = True\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import rdflib\n",
    "\n",
    "from modules import file_utils\n",
    "from modules import potential\n",
    "from modules import rdf_utils\n",
    "from modules import queries\n",
    "from modules.surprise_score import SelectionCriteria, SelectionMode\n",
    "from modules.query_runner import QueryRunner\n",
    "\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_results_stats(results_df: pd.DataFrame) -> None:\n",
    "    \"\"\"Print how many rows have empty surprising concept sets.\"\"\"\n",
    "    def _is_empty(value):\n",
    "        if isinstance(value, (list, tuple, set)):\n",
    "            return len(value) == 0\n",
    "        if isinstance(value, str):\n",
    "            return value.strip() == \"\"\n",
    "        if value is None:\n",
    "            return True\n",
    "        return bool(pd.isna(value))\n",
    "\n",
    "    empty_surprising = results_df['surprising_concepts'].apply(_is_empty).sum()\n",
    "    print(f\"Empty surprising_concepts rows: {empty_surprising} out of {len(results_df)}\")\n",
    "\n",
    "\n",
    "def _strip_prefix(items):\n",
    "    \"\"\"Return sorted concept identifiers without the core namespace prefix.\"\"\"\n",
    "    return [concept.replace(\"https://www.foom.com/core#\", \"\") for concept in sorted(items)]\n",
    "\n",
    "\n",
    "def _extract_proposition_number(label: str) -> int | None:\n",
    "    \"\"\"Extract the first integer found in a proposition label, if any.\"\"\"\n",
    "    if not isinstance(label, str):\n",
    "        return None\n",
    "    match = re.search(r\"\\d+\", label)\n",
    "    if match is None:\n",
    "        return None\n",
    "    try:\n",
    "        return int(match.group(0))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _ensure_runner(graph: rdflib.Graph | None, runner: QueryRunner | None) -> QueryRunner:\n",
    "    \"\"\"Return a QueryRunner, creating one from the supplied graph when missing.\"\"\"\n",
    "    if graph is None:\n",
    "        raise ValueError(\"A graph instance is required to run analyse().\")\n",
    "    return runner or QueryRunner(graph)\n",
    "\n",
    "\n",
    "def _fetch_proposition_types(runner: QueryRunner) -> dict[int, str]:\n",
    "    \"\"\"Build a map from proposition numbers to their type labels via SPARQL.\"\"\"\n",
    "    proposition_types_df = runner.fetch(queries.find_proposition_types())\n",
    "    proposition_types_map: dict[int, str] = {}\n",
    "    if proposition_types_df.empty:\n",
    "        return proposition_types_map\n",
    "    for _, row in proposition_types_df.iterrows():\n",
    "        label = row.get(\"proposition_pref_label\")\n",
    "        type_label = row.get(\"proposition_type_pref_label\")\n",
    "        number = _extract_proposition_number(label)\n",
    "        if number is not None and isinstance(type_label, str):\n",
    "            proposition_types_map[number] = type_label\n",
    "    return proposition_types_map\n",
    "\n",
    "\n",
    "def _compute_proposition_row(\n",
    "    graph: rdflib.Graph,\n",
    "    proposition: int,\n",
    "    *,\n",
    "    history_weights: tuple[float, float, float, float],\n",
    "    history_selection: SelectionCriteria,\n",
    "    cooccurrence_selection: SelectionCriteria,\n",
    "    runner: QueryRunner,\n",
    "    type_selection: bool,\n",
    "    proposition_type: str,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"Compute analysis artefacts for one proposition and return row data plus CSV payload.\"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Analysing proposition {proposition}\")\n",
    "\n",
    "    background_concepts, surprising = potential.main(\n",
    "        graph,\n",
    "        proposition,\n",
    "        history_weights=history_weights,\n",
    "        history_selection=history_selection,\n",
    "        cooccurrence_selection=cooccurrence_selection,\n",
    "        runner=runner,\n",
    "        type_selection=type_selection,\n",
    "    )\n",
    "\n",
    "    background_list = _strip_prefix(concept for concept in background_concepts if concept)\n",
    "    surprising_list = _strip_prefix(concept for concept in surprising if concept)\n",
    "    ratio_surprising = (\n",
    "        len(surprising_list) / len(background_list)\n",
    "        if len(background_list) > 0\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    row_dict = {\n",
    "        \"proposition\": proposition,\n",
    "        \"background_concepts\": background_list,\n",
    "        \"surprising_concepts\": surprising_list,\n",
    "        \"proposition_type\": proposition_type,\n",
    "        \"number_of_background_concepts\": len(background_list),\n",
    "        \"number_of_surprising_concepts\": len(surprising_list),\n",
    "        \"ratio_surprising_over_background\": ratio_surprising,\n",
    "    }\n",
    "\n",
    "    output_row = (\n",
    "        str(proposition),\n",
    "        background_list,\n",
    "        surprising_list,\n",
    "        proposition_type,\n",
    "        len(background_list),\n",
    "        len(surprising_list),\n",
    "        ratio_surprising,\n",
    "    )\n",
    "    return row_dict, output_row\n",
    "\n",
    "\n",
    "def _format_value(value: float) -> str:\n",
    "    \"\"\"Format numeric values for filenames, trimming redundant zeroes.\"\"\"\n",
    "    if isinstance(value, float):\n",
    "        return f\"{value:.4f}\".rstrip(\"0\").rstrip(\".\")\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "def _build_run_description(\n",
    "    history_selection: SelectionCriteria,\n",
    "    cooccurrence_selection: SelectionCriteria,\n",
    "    history_weights: tuple[float, float, float, float],\n",
    "    type_selection: bool,\n",
    ") -> str:\n",
    "    \"\"\"Compose the descriptor token that names analysis outputs.\"\"\"\n",
    "    parts = [\n",
    "        f\"history-{history_selection.mode.value}-{_format_value(history_selection.value)}\",\n",
    "        f\"coocc-{cooccurrence_selection.mode.value}-{_format_value(cooccurrence_selection.value)}\",\n",
    "        \"weights-\" + \"-\".join(_format_value(weight) for weight in history_weights),\n",
    "        \"type\" if type_selection else \"no-type\",\n",
    "    ]\n",
    "    return \"__\".join(parts)\n",
    "\n",
    "\n",
    "def _write_outputs(output_rows, description: str) -> None:\n",
    "    \"\"\"Persist the collected rows to a timestamped CSV under the output directory.\"\"\"\n",
    "    output_dir = NOTEBOOK_DIR / \"output\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_path = output_dir / f\"analyses_{description}\"\n",
    "    output_df(\n",
    "        output_rows,\n",
    "        filename=str(output_path),\n",
    "        columns=[\n",
    "            \"proposition\",\n",
    "            \"background_concepts\",\n",
    "            \"surprising_concepts\",\n",
    "            \"proposition_type\",\n",
    "            \"number_of_background_concepts\",\n",
    "            \"number_of_surprising_concepts\",\n",
    "            \"ratio_surprising_over_background\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def analyse(\n",
    "    upper_proposition_number: int,\n",
    "    history_weights: tuple[float, float, float, float],\n",
    "    history_selection: SelectionCriteria,\n",
    "    cooccurrence_selection: SelectionCriteria,\n",
    "    verbose: bool = False,\n",
    "    graph: rdflib.Graph | None = None,\n",
    "    runner: QueryRunner | None = None,\n",
    "    type_selection: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Run the surprise analysis up to the requested proposition and return a dataframe.\"\"\"\n",
    "    # Ensure we have a runner bound to the supplied graph\n",
    "    runner = _ensure_runner(graph, runner)\n",
    "    # Pre-load proposition type labels for contextual reporting\n",
    "    proposition_types = _fetch_proposition_types(runner)\n",
    "\n",
    "    # Collect notebook-friendly dictionaries for the final dataframe\n",
    "    analyses = []\n",
    "    # Hold raw tuples destined for the CSV export\n",
    "    output_rows = []\n",
    "    # Process each proposition sequentially\n",
    "    for proposition in range(1, upper_proposition_number + 1):\n",
    "        # Compute activation/surprise data for this proposition\n",
    "        row_dict, output_row = _compute_proposition_row(\n",
    "            graph,\n",
    "            proposition,\n",
    "            history_weights=history_weights,\n",
    "            history_selection=history_selection,\n",
    "            cooccurrence_selection=cooccurrence_selection,\n",
    "            runner=runner,\n",
    "            type_selection=type_selection,\n",
    "            proposition_type=proposition_types.get(proposition, \"\"),\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        # Keep structured data for dataframe assembly\n",
    "        analyses.append(row_dict)\n",
    "        # Keep tuple payload for CSV persistence\n",
    "        output_rows.append(output_row)\n",
    "\n",
    "    # Describe this run so outputs get unique names\n",
    "    description = _build_run_description(\n",
    "        history_selection,\n",
    "        cooccurrence_selection,\n",
    "        history_weights,\n",
    "        type_selection,\n",
    "    )\n",
    "    # Save the CSV snapshot under the run descriptor\n",
    "    _write_outputs(output_rows, description)\n",
    "\n",
    "    # Convert accumulated rows into a dataframe\n",
    "    results_df = pd.DataFrame(analyses)\n",
    "    # Report quick stats to the console\n",
    "    print_results_stats(results_df)\n",
    "    # Return results for further notebook analysis\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TTL = file_utils.latest_file(folder=NOTEBOOK_DIR / \"ontologies\", filename_fragment=\"ontology_\", extension=\"ttl\")\n",
    "graph = load_graph(INPUT_TTL)\n",
    "runner = QueryRunner(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_HISTORY_SELECTION = SelectionCriteria(SelectionMode.TOP_FRACTION, 1 / 1)\n",
    "DEFAULT_COOCCURRENCE_SELECTION = SelectionCriteria(SelectionMode.TOP_FRACTION, 1 / 1)\n",
    "DEFAULT_HISTORY_WEIGHTS = (6 / 9, 1 / 9, 1 / 9, 1 / 9)\n",
    "\n",
    "results_df = analyse(\n",
    "    upper_proposition_number=48,\n",
    "    history_selection=DEFAULT_HISTORY_SELECTION,\n",
    "    cooccurrence_selection=DEFAULT_COOCCURRENCE_SELECTION,\n",
    "    history_weights=DEFAULT_HISTORY_WEIGHTS,\n",
    "    verbose=True,\n",
    "    graph=graph,\n",
    "    runner=runner,\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_HISTORY_SELECTION = SelectionCriteria(SelectionMode.TOP_FRACTION, 1 / 1)\n",
    "DEFAULT_COOCCURRENCE_SELECTION = SelectionCriteria(SelectionMode.TOP_FRACTION, 1 / 1)\n",
    "DEFAULT_HISTORY_WEIGHTS = (6 / 9, 1 / 9, 2 / 9)\n",
    "\n",
    "results_df = analyse(\n",
    "    upper_proposition_number=48,\n",
    "    history_selection=DEFAULT_HISTORY_SELECTION,\n",
    "    cooccurrence_selection=DEFAULT_COOCCURRENCE_SELECTION,\n",
    "    history_weights=DEFAULT_HISTORY_WEIGHTS,\n",
    "    verbose=True,\n",
    "    graph=graph,\n",
    "    runner=runner,\n",
    "    type_selection=True\n",
    ")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
