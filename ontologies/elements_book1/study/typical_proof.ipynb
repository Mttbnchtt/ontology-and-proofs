{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5890d888",
   "metadata": {},
   "source": [
    "# Typical activation potential: analysis of each proof N against context\n",
    "\n",
    "This notebook implements the *typical activation potential* from `main.tex`.\n",
    "\n",
    "Notation (from `main.tex`):\n",
    "- Phi_T(r, C) is the typical activation potential for resource r in context C.\n",
    "- Phi_h(r, C) is the historical component of activation potential.\n",
    "- Phi_Tc(r, C) is the typical co-occurrence component (Phi_{T_c}).\n",
    "- delta (DELTA in code) is the weight in Phi_T = delta * Phi_h + (1 - delta) * Phi_Tc, with delta in [0, 1].\n",
    "\n",
    "Assumptions (matching the existing SPARQL queries):\n",
    "\n",
    "Assumption: The SPARQL queries used in this notebook were verified to implement the paper's definitions of\n",
    "context C and \"together\" (per definition/postulate/common notion or proposition/proof).\n",
    "If query scopes are changed, results may no longer match the formulas in `main.tex`.\n",
    "- Context C for proof n are the resources in definitions, postulates, common notions, propositions up to n (included), and proofs up to n-1 (included)\n",
    "- \"Together\" for co-occurrence means resources that co-occur within the same definition, postulate, common notion, proposition, or proof.\n",
    "- Phi_h is computed from history queries; Phi_Tc is computed from Hebbian pair degrees (co-occurrence links); Phi_T uses the weighted sum above.\n",
    "- Empty denominators yield 0 for the corresponding potential.\n",
    "- TYPE_SELECTION toggles type-based co-occurrence for propositions/proofs (relation/operation types) when true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6925534",
   "metadata": {},
   "source": [
    "# SPEC\n",
    "\n",
    "GOALS: \n",
    "(1) Apply the description of typical activation potential provided in main.tex to compute the activation potential of resources used in proof N against the context of resources used in definitions, postulates, common notions, propositions up to N (included), and proofs up to N-1 (included; if N=1, then there is no proof to include in the context). \n",
    "(2) Note when proof N uses _directly_ a resource that has not appeared anywhere in the context (i.e. in the definitions, postulates, common notions, propositions up to N included, and proofs up to N-1 included). For proof N, use direct_template_propositions_proofs if TYPE_SELECTION = False and direct_template_last_item_types if TYPE_SELECTION = True. Define new_resources as the subset of the direct‑usage set from the ‘HOW TO FIND RESOURCES IN PROOF N’ step that does not occur in the context.\n",
    "\n",
    "PREPARATION: review \n",
    "    - main.tex (for the definition of typical activation potential), \n",
    "    - analyses.ipynb (for strategies to compare proofs with their context of previous material), and \n",
    "    - typical.ipynb (for an algorithm implementing typical activation potential)\n",
    "    - ./modules/queries.py (for SPARQL queries used to work with ontological resources)\n",
    "\n",
    "NOTES:\n",
    "    - cache results of SPARQL queries for faster re-runs (QueryRunner)\n",
    "\n",
    "INPUTS: PROOF_N, DELTA, HISTORY_WEIGHTS (exactly 3 weights required), TYPE_SELECTION\n",
    "\n",
    "HOW TO CREATE THE CONTEXT OF PROOF N:\n",
    "- directly used resources: use queries.direct_definitions(), queries.direct_postulates(), queries.direct_common_notions(), and queries.direct_template_propositions_proofs() for propositions up to N (included) and proofs up to N-1 (included) [for propositions and proofs, one need to state the IRIs as VALUES in the SPARQL queries]\n",
    "- hierarchically used resources: use queries.hierarchical_definitions(), queries.hierarchical_postulates(), queries.hierarchical_common_notions(), and hierarchical_template_propositions_proofs for propositions up to N (included) and proofs up to N-1 (included) [for propositions and proofs, one need to state the IRIs as VALUES in the SPARQL queries]\n",
    "- mereologically used resources: use queries.mereological_definitions(), queries.mereological_postulates(), queries.mereological_common_notions, queries.mereological_template_propositions_proofs() for propositions up to N (included) and proofs up to N-1 (included) [for propositions and proofs, one need to state the IRIs as VALUES in the SPARQL queries]\n",
    "- hebbian co-occurrence: use queries.hebb_definitions(), queries.hebb_postulates(), queries.hebb_common_notions(), and queries.hebb_template_propositions_proofs() for propositions up to N (included) and proofs up to N-1 (included) [for propositions and proofs, one need to state the IRIs as VALUES in the SPARQL queries].\n",
    "\n",
    "EDGE CASE:\n",
    "- N = 1: the context includes only definitions, postulates, common notions, and proposition 1.\n",
    "\n",
    "HOW TO FIND RESOURCES IN PROOF N:\n",
    "- if TYPE_SELECTION = False: use queries.direct_template_propositions_proofs() for proof N (the list of values for the query should contain only the IRI of proof N)\n",
    "- if TYPE_SELECTION = True: use queries.direct_template_last_item_types() for proof N (the list of values for the query should contain only the IRI of proof N)\n",
    "\n",
    "OUTPUT: \n",
    "    - csv with columns \"proof\", \"resource_used_in_proof\", \"number_of_resources_used_in_proof\", \"phi_h\", \"phi_tc\", \"phi_t\", \"new_resources\", \"number_of_new_resources\"\n",
    "    - include only resources used in proof N (for each N) in the column \"resource_used_in_proof\"\n",
    "    - output path: put the csv file in the \"./output\" folder\n",
    "    - output naming convention: typical_weights-<history_weights>_type-<true_or_false>_<timestamp>.csv\n",
    "    - \"number_of_resources_used_in_proof\" is a column of scalars that counts how many resources a proof contains (set-size, not multiplicity; it is a per-proof piece of data but we keep it in this csv)\n",
    "    - \"new_resources\" contains a list of resources that are new in proof N (never used before)\n",
    "    - \"new_resources\", \"number_of_new_resources\" are per-proof data; however I want to include them in a single csv; therefore, it is ok to repeat these data on several rows for the same proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a00b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from modules import rdf_utils, file_utils\n",
    "from modules.query_runner import QueryRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec7053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0: define parameters\n",
    "DELTA = 0.5  # delta in Phi_T = delta * Phi_h + (1 - delta) * Phi_Tc\n",
    "HISTORY_WEIGHTS = (6 / 9, 1 / 9, 2 / 9)  # Phi_h weights: direct, hierarchical, mereological\n",
    "TYPE_SELECTION = False  # toggle type-based co-occurrence in propositions/proofs\n",
    "\n",
    "# NOTE: Although the SPEC lists PROOF_N as an input, \n",
    "# this notebook runs a batch analysis by iterating over a range of proofs. \n",
    "# We therefore use START_PROPOSITION/END_PROPOSITION and \n",
    "# treat each iteration as the current PROOF_N\n",
    "START_PROPOSITION = 1\n",
    "END_PROPOSITION = 48\n",
    "\n",
    "def validate_params() -> None:\n",
    "    if not (0.0 <= DELTA <= 1.0):\n",
    "        raise ValueError(f\"DELTA must be in [0, 1], got {DELTA}.\")\n",
    "    if len(HISTORY_WEIGHTS) != 3:\n",
    "        raise ValueError(\n",
    "            f\"HISTORY_WEIGHTS must have length 3, got {len(HISTORY_WEIGHTS)}.\"\n",
    "        )\n",
    "    if any((w < 0.0 or w > 1.0) for w in HISTORY_WEIGHTS):\n",
    "        raise ValueError(\"All HISTORY_WEIGHTS must be in [0, 1].\")\n",
    "    total = sum(HISTORY_WEIGHTS)\n",
    "    if abs(total - 1.0) > 1e-9:\n",
    "        raise ValueError(f\"HISTORY_WEIGHTS must sum to 1, got {total}.\")\n",
    "\n",
    "validate_params()\n",
    "\n",
    "OUTPUT_DIR = Path('output')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Load latest ontology TTL and reuse a cached QueryRunner\n",
    "INPUT_TTL = file_utils.latest_file(folder=Path('ontologies'), filename_fragment='ontology_', extension='ttl')\n",
    "graph = rdf_utils.load_graph(INPUT_TTL)\n",
    "runner = QueryRunner(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624dc94a",
   "metadata": {},
   "source": [
    "Note on reuse: `rdf_utils.sparql_to_concat_df` could replace the local aggregation helpers because it is query-agnostic,\n",
    "but `calculate_activation_potential.hebb` returns pair-level potentials (this notebook needs per-resource degrees),\n",
    "and `calculate_activation_potential.history` scopes propositions/proofs up to N-1 and does not support type selection,\n",
    "so they are not drop-in fits for this spec.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: function(s) to create context for proof N\n",
    "# - Implement a function that, for a given N, builds the context set C.\n",
    "# - Use the SPEC’s query families: direct, hierarchical, mereological, hebbian.\n",
    "# - Apply each family per source type: definitions, postulates, common notions, propositions, proofs.\n",
    "# - Collect hebbian co-occurrence data here (queries.hebb_*) for Phi_Tc.\n",
    "# - Sources: definitions, postulates, common notions (all); propositions <= N; proofs <= N-1 (none if N=1).\n",
    "# - For propositions/proofs, pass explicit IRI lists as VALUES to the queries.\n",
    "# - Return de-duplicated context resources plus the hebb co-occurrence set.\n",
    "# - Keep it pure: no printing, no file I/O; just compute and return.\n",
    "\n",
    "from typing import Iterable\n",
    "\n",
    "from modules import queries\n",
    "\n",
    "\n",
    "def _iri_for_proposition(proof_n: int) -> str:\n",
    "    return f\"<https://www.foom.com/core#proposition_{proof_n}>\"\n",
    "\n",
    "\n",
    "def _iri_for_proof(proof_n: int) -> str:\n",
    "    return f\"<https://www.foom.com/core#proof_{proof_n}>\"\n",
    "\n",
    "\n",
    "def _iris_for_context(proof_n: int) -> list[str]:\n",
    "    propositions = [_iri_for_proposition(i) for i in range(1, proof_n + 1)]\n",
    "    proofs = [_iri_for_proof(i) for i in range(1, proof_n)]\n",
    "    return propositions + proofs\n",
    "\n",
    "\n",
    "def _values_clause(values: Iterable[str]) -> str | None:\n",
    "    tokens = [value for value in values if value]\n",
    "    if not tokens:\n",
    "        return None\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def _fetch_sum_links(runner: QueryRunner, queries_to_run: Iterable[str]) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for query in queries_to_run:\n",
    "        df = runner.fetch(query)\n",
    "        if df.empty or \"o\" not in df.columns:\n",
    "            continue\n",
    "        if \"links\" in df.columns:\n",
    "            frame = df[[\"o\", \"links\"]].copy()\n",
    "        else:\n",
    "            frame = df[[\"o\"]].copy()\n",
    "            frame[\"links\"] = 1\n",
    "        frames.append(frame)\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=[\"o\", \"links\"])\n",
    "    return (\n",
    "        pd.concat(frames, ignore_index=True)\n",
    "        .groupby(\"o\", as_index=False)[\"links\"]\n",
    "        .sum()\n",
    "    )\n",
    "\n",
    "\n",
    "def _fetch_hebb_links(runner: QueryRunner, queries_to_run: Iterable[str]) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for query in queries_to_run:\n",
    "        df = runner.fetch(query)\n",
    "        if df.empty or \"o1\" not in df.columns or \"o2\" not in df.columns:\n",
    "            continue\n",
    "        if \"links\" in df.columns:\n",
    "            frame = df[[\"o1\", \"o2\", \"links\"]].copy()\n",
    "        else:\n",
    "            frame = df[[\"o1\", \"o2\"]].copy()\n",
    "            frame[\"links\"] = 1\n",
    "        frames.append(frame)\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=[\"o1\", \"o2\", \"links\"])\n",
    "    return (\n",
    "        pd.concat(frames, ignore_index=True)\n",
    "        .groupby([\"o1\", \"o2\"], as_index=False)[\"links\"]\n",
    "        .sum()\n",
    "    )\n",
    "\n",
    "\n",
    "def build_context_for_proof(\n",
    "    proof_n: int,\n",
    "    *,\n",
    "    runner: QueryRunner,\n",
    "    type_selection: bool,\n",
    ") -> tuple[set[str], dict[str, pd.DataFrame], pd.DataFrame]:\n",
    "    values = _values_clause(_iris_for_context(proof_n))\n",
    "\n",
    "    direct_queries = [\n",
    "        queries.direct_definitions(),\n",
    "        queries.direct_postulates(),\n",
    "        queries.direct_common_notions(),\n",
    "    ]\n",
    "    if values:\n",
    "        direct_queries.append(queries.direct_template_propositions_proofs(values))\n",
    "    direct_df = _fetch_sum_links(runner, direct_queries)\n",
    "\n",
    "    hierarchical_queries = [\n",
    "        queries.hierarchical_definitions(),\n",
    "        queries.hierarchical_postulates(),\n",
    "        queries.hierarchical_common_notions(),\n",
    "    ]\n",
    "    if values:\n",
    "        hierarchical_queries.append(queries.hierarchical_template_propositions_proofs(values))\n",
    "    hierarchical_df = _fetch_sum_links(runner, hierarchical_queries)\n",
    "\n",
    "    mereological_queries = [\n",
    "        queries.mereological_definitions(),\n",
    "        queries.mereological_postulates(),\n",
    "        queries.mereological_common_notions(),\n",
    "    ]\n",
    "    if values:\n",
    "        mereological_queries.append(queries.mereological_template_propositions_proofs(values))\n",
    "    mereological_df = _fetch_sum_links(runner, mereological_queries)\n",
    "\n",
    "    hebb_queries = [\n",
    "        queries.hebb_definitions(),\n",
    "        queries.hebb_postulates(),\n",
    "        queries.hebb_common_notions(),\n",
    "    ]\n",
    "    if values:\n",
    "        if type_selection:\n",
    "            hebb_queries.append(queries.hebb_template_propositions_proofs_types(values))\n",
    "        else:\n",
    "            hebb_queries.append(queries.hebb_template_propositions_proofs(values))\n",
    "    hebb_df = _fetch_hebb_links(runner, hebb_queries)\n",
    "\n",
    "    context_resources: set[str] = set()\n",
    "    for df in (direct_df, hierarchical_df, mereological_df):\n",
    "        if not df.empty and \"o\" in df.columns:\n",
    "            context_resources.update(df[\"o\"].dropna().astype(str))\n",
    "\n",
    "    family_dfs = {\n",
    "        \"direct\": direct_df,\n",
    "        \"hierarchical\": hierarchical_df,\n",
    "        \"mereological\": mereological_df,\n",
    "    }\n",
    "    return context_resources, family_dfs, hebb_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60333ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: function(s) to gather resources from proof N\n",
    "# - Implement a function that, for a given N, returns the direct-usage set for proof N.\n",
    "# - If TYPE_SELECTION is False: use queries.direct_template_propositions_proofs() on proof N.\n",
    "# - If TYPE_SELECTION is True: use queries.direct_template_last_item_types() on proof N.\n",
    "# - Pass a single proof IRI as VALUES.\n",
    "# - Keep it pure: no printing, no file I/O; just compute and return.\n",
    "\n",
    "def resources_in_proof(\n",
    "    proof_n: int,\n",
    "    *,\n",
    "    runner: QueryRunner,\n",
    "    type_selection: bool,\n",
    ") -> set[str]:\n",
    "    proof_iri = _iri_for_proof(proof_n)\n",
    "    if type_selection:\n",
    "        query = queries.direct_template_last_item_types(proof_iri)\n",
    "    else:\n",
    "        query = queries.direct_template_propositions_proofs(proof_iri)\n",
    "    df = runner.fetch(query)\n",
    "    if \"o\" not in df.columns:\n",
    "        return set()\n",
    "    return {str(value) for value in df[\"o\"].dropna().astype(str)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9720d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: function(s) to calculate the historical activation potential Phi_h\n",
    "# - Implement a function that computes Phi_h for each resource used in proof N.\n",
    "# - Use HISTORY_WEIGHTS for direct/hierarchical/mereological components per SPEC.\n",
    "# - Compare only against the context built in STEP 2 for the same N.\n",
    "# - If a denominator is empty, return 0 for that component.\n",
    "# - Keep it pure: no printing, no file I/O; just compute and return.\n",
    "\n",
    "def compute_phi_h(\n",
    "    proof_resources: Iterable[str],\n",
    "    family_dfs: dict[str, pd.DataFrame],\n",
    "    weights: tuple[float, float, float],\n",
    ") -> pd.DataFrame:\n",
    "    resources = sorted(set(proof_resources))\n",
    "    if not resources:\n",
    "        return pd.DataFrame(columns=[\"resource_used_in_proof\", \"phi_h\"])\n",
    "\n",
    "    phi_h = {resource: 0.0 for resource in resources}\n",
    "    for family_name, weight in zip(\n",
    "        (\"direct\", \"hierarchical\", \"mereological\"),\n",
    "        weights,\n",
    "    ):\n",
    "        df = family_dfs.get(family_name, pd.DataFrame())\n",
    "        if df.empty or \"links\" not in df.columns:\n",
    "            continue\n",
    "        total_links = float(df[\"links\"].sum())\n",
    "        if total_links == 0:\n",
    "            continue\n",
    "        link_map = {\n",
    "            str(row[\"o\"]): float(row[\"links\"])\n",
    "            for _, row in df.iterrows()\n",
    "        }\n",
    "        for resource in resources:\n",
    "            links = link_map.get(resource, 0.0)\n",
    "            if links:\n",
    "                phi_h[resource] += (links * weight) / total_links\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"resource_used_in_proof\": resources,\n",
    "        \"phi_h\": [phi_h[resource] for resource in resources],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a052a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: function(s) to calculate the co-occurrence activation potential Phi_Tc\n",
    "# - Implement a function that computes Phi_Tc for each resource used in proof N.\n",
    "# - Use hebbian co-occurrence data from STEP 2 (queries.hebb_*).\n",
    "# - Compare only against the context built in STEP 2 for the same N.\n",
    "# - If a denominator is empty, return 0.\n",
    "# - Keep it pure: no printing, no file I/O; just compute and return.\n",
    "\n",
    "def compute_phi_tc(\n",
    "    proof_resources: Iterable[str],\n",
    "    hebb_df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    resources = sorted(set(proof_resources))\n",
    "    if not resources:\n",
    "        return pd.DataFrame(columns=[\"resource_used_in_proof\", \"phi_tc\"])\n",
    "\n",
    "    degrees: dict[str, float] = {}\n",
    "    if not hebb_df.empty and \"links\" in hebb_df.columns:\n",
    "        for _, row in hebb_df.iterrows():\n",
    "            o1 = str(row[\"o1\"])\n",
    "            o2 = str(row[\"o2\"])\n",
    "            weight = float(row[\"links\"])\n",
    "            degrees[o1] = degrees.get(o1, 0.0) + weight\n",
    "            degrees[o2] = degrees.get(o2, 0.0) + weight\n",
    "\n",
    "    total_degree = sum(degrees.values())\n",
    "    if total_degree == 0:\n",
    "        phi_tc = {resource: 0.0 for resource in resources}\n",
    "    else:\n",
    "        phi_tc = {\n",
    "            resource: degrees.get(resource, 0.0) / total_degree\n",
    "            for resource in resources\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"resource_used_in_proof\": resources,\n",
    "        \"phi_tc\": [phi_tc[resource] for resource in resources],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec8c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: function(s) to calculate the total activation potential Phi_T\n",
    "# - Implement a function that combines Phi_h and Phi_Tc.\n",
    "# - Phi_T = DELTA * Phi_h + (1 - DELTA) * Phi_Tc.\n",
    "# - Compute only for resources used in proof N.\n",
    "# - Keep it pure: no printing, no file I/O; just compute and return.\n",
    "\n",
    "def compute_phi_t(\n",
    "    proof_resources: Iterable[str],\n",
    "    phi_h_df: pd.DataFrame,\n",
    "    phi_tc_df: pd.DataFrame,\n",
    "    delta: float,\n",
    ") -> pd.DataFrame:\n",
    "    resources = sorted(set(proof_resources))\n",
    "    if not resources:\n",
    "        return pd.DataFrame(columns=[\"resource_used_in_proof\", \"phi_t\"])\n",
    "\n",
    "    phi_h_map = {}\n",
    "    if not phi_h_df.empty:\n",
    "        phi_h_map = dict(zip(phi_h_df[\"resource_used_in_proof\"], phi_h_df[\"phi_h\"]))\n",
    "    phi_tc_map = {}\n",
    "    if not phi_tc_df.empty:\n",
    "        phi_tc_map = dict(zip(phi_tc_df[\"resource_used_in_proof\"], phi_tc_df[\"phi_tc\"]))\n",
    "\n",
    "    phi_t = {\n",
    "        resource: delta * float(phi_h_map.get(resource, 0.0))\n",
    "        + (1 - delta) * float(phi_tc_map.get(resource, 0.0))\n",
    "        for resource in resources\n",
    "    }\n",
    "    return pd.DataFrame({\n",
    "        \"resource_used_in_proof\": resources,\n",
    "        \"phi_t\": [phi_t[resource] for resource in resources],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: function(s) to find resources in proof N that are not in the context\n",
    "# - Implement a function that computes new_resources for proof N.\n",
    "# - new_resources = direct-usage set from STEP 3 minus context from STEP 2.\n",
    "# - Return the list plus a count for output convenience.\n",
    "# - Keep it pure: no printing, no file I/O; just compute and return.\n",
    "\n",
    "def compute_new_resources(\n",
    "    proof_resources: Iterable[str],\n",
    "    context_resources: Iterable[str],\n",
    ") -> list[str]:\n",
    "    proof_set = set(proof_resources)\n",
    "    context_set = set(context_resources)\n",
    "    return sorted(proof_set - context_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84dadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: run a loop with all steps for proofs N = START_PROPOSITION to END_PROPOSITION\n",
    "# - For each N, build context (STEP 2), proof resources (STEP 3), Phi_h (STEP 4), Phi_Tc (STEP 5), Phi_T (STEP 6), new_resources (STEP 7).\n",
    "# - Handle the N=1 edge case (no proofs in context).\n",
    "# - Accumulate row data for each resource used in proof N.\n",
    "\n",
    "results_rows: list[dict[str, object]] = []\n",
    "for proof_n in range(START_PROPOSITION, END_PROPOSITION + 1):\n",
    "    context_resources, family_dfs, hebb_df = build_context_for_proof(\n",
    "        proof_n,\n",
    "        runner=runner,\n",
    "        type_selection=TYPE_SELECTION,\n",
    "    )\n",
    "    proof_resources = resources_in_proof(\n",
    "        proof_n,\n",
    "        runner=runner,\n",
    "        type_selection=TYPE_SELECTION,\n",
    "    )\n",
    "    proof_resources_sorted = sorted(proof_resources)\n",
    "\n",
    "    phi_h_df = compute_phi_h(proof_resources_sorted, family_dfs, HISTORY_WEIGHTS)\n",
    "    phi_tc_df = compute_phi_tc(proof_resources_sorted, hebb_df)\n",
    "    phi_t_df = compute_phi_t(proof_resources_sorted, phi_h_df, phi_tc_df, DELTA)\n",
    "\n",
    "    new_resources = compute_new_resources(proof_resources_sorted, context_resources)\n",
    "    proof_count = len(proof_resources_sorted)\n",
    "    new_count = len(new_resources)\n",
    "\n",
    "    phi_h_map = dict(zip(phi_h_df[\"resource_used_in_proof\"], phi_h_df[\"phi_h\"]))\n",
    "    phi_tc_map = dict(zip(phi_tc_df[\"resource_used_in_proof\"], phi_tc_df[\"phi_tc\"]))\n",
    "    phi_t_map = dict(zip(phi_t_df[\"resource_used_in_proof\"], phi_t_df[\"phi_t\"]))\n",
    "\n",
    "    for resource in proof_resources_sorted:\n",
    "        results_rows.append({\n",
    "            \"proof\": proof_n,\n",
    "            \"resource_used_in_proof\": resource,\n",
    "            \"number_of_resources_used_in_proof\": proof_count,\n",
    "            \"phi_h\": float(phi_h_map.get(resource, 0.0)),\n",
    "            \"phi_tc\": float(phi_tc_map.get(resource, 0.0)),\n",
    "            \"phi_t\": float(phi_t_map.get(resource, 0.0)),\n",
    "            \"new_resources\": new_resources,\n",
    "            \"number_of_new_resources\": new_count,\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 9: print df of the results; print a summary of the results; save results to CSV\n",
    "# - Build a DataFrame with columns: proof, resource_used_in_proof, number_of_resources_used_in_proof, phi_h, phi_tc, phi_t, new_resources, number_of_new_resources.\n",
    "# - Repeat per-proof scalars (counts, new_resources) across each resource row.\n",
    "# - Serialize history_weights as hyphen-joined decimals (e.g., 0.6667-0.1111-0.2222).\n",
    "# - Use timestamp format YYYYMMDD-HHMMSS and save to ./output as:\n",
    "#   typical_weights-<history_weights>_type-<true_or_false>_<timestamp>.csv\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    results_rows,\n",
    "    columns=[\n",
    "        \"proof\",\n",
    "        \"resource_used_in_proof\",\n",
    "        \"number_of_resources_used_in_proof\",\n",
    "        \"phi_h\",\n",
    "        \"phi_tc\",\n",
    "        \"phi_t\",\n",
    "        \"new_resources\",\n",
    "        \"number_of_new_resources\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "history_weights_label = \"-\".join(f\"{w:.4f}\" for w in HISTORY_WEIGHTS)\n",
    "timestamp = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "type_label = str(TYPE_SELECTION).lower()\n",
    "output_path = OUTPUT_DIR / f\"typical_weights-{history_weights_label}_type-{type_label}_{timestamp}.csv\"\n",
    "\n",
    "print(results_df)\n",
    "print(f\"Total proofs processed: {END_PROPOSITION - START_PROPOSITION + 1}\")\n",
    "print(f\"Total rows: {len(results_df)}\")\n",
    "\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad50a8",
   "metadata": {},
   "source": [
    "Note on TYPE_SELECTION: in analyses.ipynb, type_selection only switches proof-level direct resources to relation/operation types,\n",
    "while history and co-occurrence stay concept-based. In typical_proof.ipynb, TYPE_SELECTION switches both proof resources\n",
    "and Hebbian co-occurrence queries to type-based variants; history context remains concept-based.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
