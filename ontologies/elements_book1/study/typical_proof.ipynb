{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5890d888",
   "metadata": {},
   "source": [
    "# Typical activation potential: analysis of each proof N against context\n",
    "\n",
    "This notebook implements the *typical activation potential* from `main.tex`.\n",
    "\n",
    "Notation (from `main.tex`):\n",
    "- Phi_T(r, C) is the typical activation potential for resource r in context C.\n",
    "- Phi_h(r, C) is the historical component of activation potential.\n",
    "- Phi_Tc(r, C) is the typical co-occurrence component (Phi_{T_c}).\n",
    "- delta (DELTA in code) is the weight in Phi_T = delta * Phi_h + (1 - delta) * Phi_Tc, with delta in [0, 1].\n",
    "\n",
    "Assumptions (matching the existing SPARQL queries):\n",
    "\n",
    "Assumption: The SPARQL queries used in this notebook were verified to implement the paper's definitions of\n",
    "context C and \"together\" (per definition/postulate/common notion or proposition/proof).\n",
    "If query scopes are changed, results may no longer match the formulas in `main.tex`.\n",
    "- Context C for proof n are the resources in definitions, postulates, common notions, propositions up to n (included), and proofs up to n-1 (included)\n",
    "- \"Together\" for co-occurrence means resources that co-occur within the same definition, postulate, common notion, proposition, or proof.\n",
    "- Phi_h is computed from history queries; Phi_Tc is computed from Hebbian pair degrees (co-occurrence links); Phi_T uses the weighted sum above.\n",
    "- Empty denominators yield 0 for the corresponding potential.\n",
    "- TYPE_SELECTION toggles type-based co-occurrence for propositions/proofs (relation/operation types) when true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6925534",
   "metadata": {},
   "source": [
    "# SPEC\n",
    "\n",
    "GOALS: \n",
    "(1) Apply the description of typical activation potential provided in main.tex to compute the activation potential of resources used in proof N against the context of resources used in definitions, postulates, common notions, propositions up to N (included), and proofs up to N-1 (included; if N=1, then there is no proof to include in the context). \n",
    "(2) Note when proof N uses _directly_ a resource that has not appeared anywhere in the context (i.e. in the definitions, postulates, common notions, propositions up to N included, and proofs up to N-1 included). For proof N, use direct_template_propositions_proofs if TYPE_SELECTION = False and direct_template_last_item_types if TYPE_SELECTION = True. Define new_resources as the subset of the direct‑usage set from the ‘HOW TO FIND RESOURCES IN PROOF N’ step that does not occur in the context.\n",
    "\n",
    "PREPARATION: review \n",
    "    - main.tex (for the definition of typical activation potential), \n",
    "    - analyses.ipynb (for strategies to compare proofs with their context of previous material), and \n",
    "    - typical.ipynb (for an algorithm implementing typical activation potential)\n",
    "    - ./modules/queries.py (for SPARQL queries used to work with ontological resources)\n",
    "\n",
    "NOTES:\n",
    "    - cache results of SPARQL queries for faster re-runs (QueryRunner)\n",
    "\n",
    "INPUTS: PROOF_N, DELTA, HISTORY_WEIGHTS (exactly 3 weights required), TYPE_SELECTION\n",
    "\n",
    "HOW TO CREATE THE CONTEXT OF PROOF N:\n",
    "- directly used resources: use queries.direct_definitions(), queries.direct_postulates(), queries.direct_common_notions(), and queries.direct_template_propositions_proofs() for propositions up to N (included) and proofs up to N-1 (included) [for propositions and proofs, one need to state the IRIs as VALUES in the SPARQL queries]\n",
    "- hierarchically used resources: use queries.hierarchical_definitions(), queries.hierarchical_postulates(), queries.hierarchical_common_notions(), and hierarchical_template_propositions_proofs for propositions up to N (included) and proofs up to N-1 (included) [for propositions and proofs, one need to state the IRIs as VALUES in the SPARQL queries]\n",
    "- mereologically used resources: use queries.mereological_definitions(), queries.mereological_postulates(), queries.mereological_common_notions, queries.mereological_template_propositions_proofs() for propositions up to N (included) and proofs up to N-1 (included) [for propositions and proofs, one need to state the IRIs as VALUES in the SPARQL queries]\n",
    "- hebbian co-occurrence: use queries.hebb_definitions(), queries.hebb_postulates(), queries.hebb_common_notions(), and queries.hebb_template_propositions_proofs() for propositions up to N (included) and proofs up to N-1 (included) [for propositions and proofs, one need to state the IRIs as VALUES in the SPARQL queries].\n",
    "\n",
    "EDGE CASE:\n",
    "- N = 1: the context includes only definitions, postulates, common notions, and proposition 1.\n",
    "\n",
    "HOW TO FIND RESOURCES IN PROOF N:\n",
    "- if TYPE_SELECTION = False: use queries.direct_template_propositions_proofs() for proof N (the list of values for the query should contain only the IRI of proof N)\n",
    "- if TYPE_SELECTION = True: use queries.direct_template_last_item_types() for proof N (the list of values for the query should contain only the IRI of proof N)\n",
    "\n",
    "OUTPUT: \n",
    "    - csv with columns \"proof\", \"resource_used_in_proof\", \"number_of_resources_used_in_proof\", \"phi_h\", \"phi_tc\", \"phi_t\", \"new_resources\", \"number_of_new_resources\"\n",
    "    - include only resources used in proof N (for each N) in the column \"resource_used_in_proof\"\n",
    "    - output path: put the csv file in the \"./output\" folder\n",
    "    - output naming convention: typical_weights-<history_weights>_type-<true_or_false>_<timestamp>.csv\n",
    "    - \"number_of_resources_used_in_proof\" is a column of scalars that counts how many resources a proof contains (it is a per-proof piece of data but we keep it in this csv)\n",
    "    - \"new_resources\" contains a list of resources that are new in proof N (never used before)\n",
    "    - \"new_resources\", \"number_of_new_resources\" are per-proof data; however I want to include them in a single csv; therefore, it is ok to repeat these data on several rows for the same proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a00b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from modules import rdf_utils, file_utils\n",
    "from modules.calculate_activation_potential import history as history_potential\n",
    "from modules.calculate_activation_potential import hebb as hebb_potential\n",
    "from modules.query_runner import QueryRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec7053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0: define parameters\n",
    "DELTA = 0.5  # delta in Phi_T = delta * Phi_h + (1 - delta) * Phi_Tc\n",
    "HISTORY_WEIGHTS = (6 / 9, 1 / 9, 2 / 9)  # Phi_h weights: direct, hierarchical, mereological\n",
    "TYPE_SELECTION = False  # toggle type-based co-occurrence in propositions/proofs\n",
    "\n",
    "# NOTE: Although the SPEC lists PROOF_N as an input, \n",
    "# this notebook runs a batch analysis by iterating over a range of proofs. \n",
    "# We therefore use START_PROPOSITION/END_PROPOSITION and \n",
    "# treat each iteration as the current PROOF_N\n",
    "START_PROPOSITION = 1\n",
    "END_PROPOSITION = 48\n",
    "\n",
    "def validate_params() -> None:\n",
    "    if not (0.0 <= DELTA <= 1.0):\n",
    "        raise ValueError(f\"DELTA must be in [0, 1], got {DELTA}.\")\n",
    "    if len(HISTORY_WEIGHTS) != 3:\n",
    "        raise ValueError(\n",
    "            f\"HISTORY_WEIGHTS must have length 3, got {len(HISTORY_WEIGHTS)}.\"\n",
    "        )\n",
    "    if any((w < 0.0 or w > 1.0) for w in HISTORY_WEIGHTS):\n",
    "        raise ValueError(\"All HISTORY_WEIGHTS must be in [0, 1].\")\n",
    "    total = sum(HISTORY_WEIGHTS)\n",
    "    if abs(total - 1.0) > 1e-9:\n",
    "        raise ValueError(f\"HISTORY_WEIGHTS must sum to 1, got {total}.\")\n",
    "\n",
    "validate_params()\n",
    "\n",
    "OUTPUT_DIR = Path('output')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Load latest ontology TTL and reuse a cached QueryRunner\n",
    "INPUT_TTL = file_utils.latest_file(folder=Path('ontologies'), filename_fragment='ontology_', extension='ttl')\n",
    "graph = rdf_utils.load_graph(INPUT_TTL)\n",
    "runner = QueryRunner(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b8acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: function(s) to create context for proof N\n",
    "# - Implement a function that, for a given N, builds the context set C.\n",
    "# - Use the SPEC’s query families: direct, hierarchical, mereological, hebb.\n",
    "# - Sources: definitions, postulates, common notions (all); propositions <= N; proofs <= N-1 (none if N=1).\n",
    "# - For propositions/proofs, pass explicit IRI lists as VALUES to the queries.\n",
    "# - Return de-duplicated context resources (and hebb pairs if needed for Phi_Tc).\n",
    "# - Keep it pure: no printing, no file I/O; just compute and return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60333ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: function(s) to gather resources from proof N\n",
    "# - SPEC is source of truth; use TYPE_SELECTION to choose the direct query.\n",
    "# - Query only proof N (single IRI in VALUES) and normalize to match context resource IDs.\n",
    "# - Create functions only. We will call them in a loop below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9720d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: function(s) to calculate the historical activation potential Phi_h for all resources in proof N\n",
    "# - SPEC is source of truth; use HISTORY_WEIGHTS and return 0 on empty denominators.\n",
    "# - Compute Phi_h only for resources in proof N against the context built in STEP 2.\n",
    "# - Create functions only. We will call them in a loop below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a052a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: function(s) to calculate the co-occurrence activation potential Phi_Tc for all resources in proof N\n",
    "# - SPEC is source of truth; use hebbian co-occurrence queries and return 0 on empty denominators.\n",
    "# - Compute Phi_Tc only for resources in proof N against the context built in STEP 2.\n",
    "# - Create functions only. We will call them in a loop below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec8c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: function(s) to calculate the total activation potential Phi_T for all resources in proof N\n",
    "# - SPEC is source of truth; Phi_T = DELTA * Phi_h + (1 - DELTA) * Phi_Tc.\n",
    "# - Create functions only. We will call them in a loop below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: function(s) to find resources in proof N that are not in the context\n",
    "# - SPEC is source of truth; new_resources = direct-usage set (STEP 3) minus context (STEP 2).\n",
    "# - Create functions only. We will call them in a loop below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84dadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: run a loop with all steps for proofs N = START_PROPOSITION to END_PROPOSITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 9: print df of the results; print a summary of the results; save results to CSV\n",
    "# - SPEC is source of truth; output columns, path, and naming convention must match.\n",
    "# - Repeat per-proof scalars (counts, new_resources) across rows for each resource.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
