{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cafd4de",
   "metadata": {},
   "source": [
    "# Typical activation potential\n",
    "\n",
    "This notebook implements the *typical activation potential* from `main.tex`.\n",
    "\n",
    "Notation (from `main.tex`):\n",
    "- Phi_T(r, C) is the typical activation potential for resource r in context C.\n",
    "- Phi_h(r, C) is the historical component of activation potential.\n",
    "- Phi_Tc(r, C) is the typical co-occurrence component (Phi_{T_c}).\n",
    "- delta (DELTA in code) is the weight in Phi_T = delta * Phi_h + (1 - delta) * Phi_Tc, with delta in [0, 1].\n",
    "\n",
    "Assumptions (matching the existing SPARQL queries):\n",
    "- Context C for proposition n is the prefix of definitions/postulates/common notions plus propositions/proofs 1..n-1.\n",
    "- \"Together\" for co-occurrence means resources that co-occur within the same definition/postulate/common notion or within the same proposition/proof.\n",
    "- Phi_h is computed from history queries; Phi_Tc is computed from Hebbian pair degrees (co-occurrence links); Phi_T uses the weighted sum above.\n",
    "- Empty denominators yield 0 for the corresponding potential.\n",
    "- TYPE_SELECTION toggles type-based co-occurrence for propositions/proofs (relation/operation types) when true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3961de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from modules import rdf_utils, file_utils\n",
    "from modules.calculate_activation_potential import history as history_potential\n",
    "from modules.calculate_activation_potential import hebb as hebb_potential\n",
    "from modules.query_runner import QueryRunner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57588cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (Phi_T uses DELTA = delta)\n",
    "DELTA = 0.5  # delta in Phi_T = delta * Phi_h + (1 - delta) * Phi_Tc\n",
    "HISTORY_WEIGHTS = (6 / 9, 1 / 9, 1 / 9, 1 / 9)  # Phi_h weights: direct, hierarchical, mereological, concept-membership\n",
    "TYPE_SELECTION = False  # toggle type-based co-occurrence in propositions/proofs\n",
    "START_PROPOSITION = 1\n",
    "END_PROPOSITION = 48\n",
    "\n",
    "OUTPUT_DIR = Path('output')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fe1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest ontology TTL and reuse a cached QueryRunner\n",
    "INPUT_TTL = file_utils.latest_file(folder=Path('ontologies'), filename_fragment='ontology_', extension='ttl')\n",
    "graph = rdf_utils.load_graph(INPUT_TTL)\n",
    "runner = QueryRunner(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a01d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phi_tc(hebb_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return Phi_Tc (typical co-occurrence component, Phi_{T_c}) per resource.\"\"\"\n",
    "    if hebb_df.empty:\n",
    "        return pd.DataFrame(columns=['o', 'phi_tc'])\n",
    "\n",
    "    degrees: dict[str, float] = {}  # aggregate Hebbian degree per resource\n",
    "    for _, row in hebb_df.iterrows():\n",
    "        o1 = str(row['o1'])  # first resource in the pair\n",
    "        o2 = str(row['o2'])  # second resource in the pair\n",
    "        weight = float(row['activation_potential'])  # normalized co-occurrence weight\n",
    "        degrees[o1] = degrees.get(o1, 0.0) + weight\n",
    "        degrees[o2] = degrees.get(o2, 0.0) + weight\n",
    "\n",
    "    total_degree = sum(degrees.values())  # denominator for degree normalization\n",
    "    if total_degree == 0:\n",
    "        return pd.DataFrame({'o': list(degrees.keys()), 'phi_tc': [0.0] * len(degrees)})\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'o': list(degrees.keys()),\n",
    "        'phi_tc': [value / total_degree for value in degrees.values()],\n",
    "    })\n",
    "\n",
    "\n",
    "def compute_phi_t_for_proposition(proposition_number: int) -> pd.DataFrame:\n",
    "    \"\"\"Compute Phi_h, Phi_Tc, and Phi_T for the proposition's context C.\"\"\"\n",
    "    # Phi_h from historical use (direct/hierarchical/mereological + concept-membership)\n",
    "    history_df = history_potential(\n",
    "        graph,\n",
    "        proposition_number,\n",
    "        weights=HISTORY_WEIGHTS,\n",
    "        runner=runner,\n",
    "    )\n",
    "\n",
    "    # Hebbian co-occurrence pairs for Phi_Tc\n",
    "    # TYPE_SELECTION=True uses relation/operation types for proposition/proof co-occurrence.\n",
    "    hebb_df = hebb_potential(graph, proposition_number, runner=runner, type_selection=TYPE_SELECTION)\n",
    "    phi_tc_df = compute_phi_tc(hebb_df)\n",
    "\n",
    "    # Align Phi_h and Phi_Tc on the same resource universe, defaulting missing to 0\n",
    "    # Each series maps resource IRI -> component value; empty inputs yield empty series.\n",
    "    phi_h = history_df.set_index('o')['activation_potential'] if not history_df.empty else pd.Series(dtype=float)  # Phi_h per resource\n",
    "    phi_tc = phi_tc_df.set_index('o')['phi_tc'] if not phi_tc_df.empty else pd.Series(dtype=float)  # Phi_Tc per resource\n",
    "\n",
    "    # Union of resources seen historically or co-occurring in context C.\n",
    "    # Sorting makes the output stable across runs for identical inputs.\n",
    "    universe = sorted(set(phi_h.index) | set(phi_tc.index))\n",
    "    # Reindex so every resource has both components; missing values become 0.\n",
    "    phi_h = phi_h.reindex(universe, fill_value=0.0)\n",
    "    phi_tc = phi_tc.reindex(universe, fill_value=0.0)  # same universe as Phi_h\n",
    "\n",
    "    # Phi_T is the convex combination from main.tex\n",
    "    phi_t = DELTA * phi_h + (1 - DELTA) * phi_tc\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'proposition': proposition_number,\n",
    "        'concept': universe,\n",
    "        'phi_h': phi_h.values,\n",
    "        'phi_tc': phi_tc.values,\n",
    "        'phi_t': phi_t.values,\n",
    "    })\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc48d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Phi_T(r, C) per proposition context C\n",
    "results = []  # collect per-proposition Phi_T tables\n",
    "for proposition in range(START_PROPOSITION, END_PROPOSITION + 1):\n",
    "    results.append(compute_phi_t_for_proposition(proposition))\n",
    "\n",
    "results_df = pd.concat(results, ignore_index=True)  # long table over propositions\n",
    "results_df\n",
    "\n",
    "# Report overall min/max Phi_T with the corresponding concepts\n",
    "# Also report the propositions (proofs) where those extremes occur.\n",
    "min_phi_t = results_df['phi_t'].min()\n",
    "max_phi_t = results_df['phi_t'].max()\n",
    "min_concepts = results_df.loc[results_df['phi_t'] == min_phi_t, 'concept'].unique()\n",
    "min_props = results_df.loc[results_df['phi_t'] == min_phi_t, 'proposition'].unique()\n",
    "max_concepts = results_df.loc[results_df['phi_t'] == max_phi_t, 'concept'].unique()\n",
    "max_props = results_df.loc[results_df['phi_t'] == max_phi_t, 'proposition'].unique()\n",
    "print(f\"Min phi_t: {min_phi_t} | concepts: {', '.join(map(str, min_concepts))} | propositions: {', '.join(map(str, sorted(min_props)))}\")\n",
    "print(f\"Max phi_t: {max_phi_t} | concepts: {', '.join(map(str, max_concepts))} | propositions: {', '.join(map(str, sorted(max_props)))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac629b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist results with parameterized filename\n",
    "weights_token = '-'.join(f'{w:.4f}'.rstrip('0').rstrip('.') for w in HISTORY_WEIGHTS)  # compact weight signature\n",
    "timestamp = dt.datetime.now().strftime('%Y%m%d_%H%M%S')  # make outputs unique and sortable\n",
    "output_path = OUTPUT_DIR / f'typical_phiT_delta-{DELTA:.3f}_weights-{weights_token}_p{START_PROPOSITION}-p{END_PROPOSITION}_{timestamp}.csv'\n",
    "results_df.to_csv(output_path, index=False)  # CSV for downstream analyses\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny sanity check: propositions 1-3, top/bottom resources by Phi_T\n",
    "for prop in range(1, min(3, END_PROPOSITION) + 1):\n",
    "    subset = results_df[results_df['proposition'] == prop].copy()\n",
    "    subset = subset.sort_values('phi_t', ascending=False)  # rank by typical activation potential\n",
    "    print(f'Proposition {prop}: top 5')\n",
    "    print(subset.head(5)[['concept', 'phi_t']].to_string(index=False))\n",
    "    print(f'Proposition {prop}: bottom 5')\n",
    "    print(subset.tail(5)[['concept', 'phi_t']].to_string(index=False))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6661e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
