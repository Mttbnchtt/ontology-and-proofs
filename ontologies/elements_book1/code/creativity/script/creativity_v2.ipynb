{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "print(\"---\")\n",
    "\n",
    "# install and import modules\n",
    "%pip install rdflib\n",
    "\n",
    "import google\n",
    "import IPython\n",
    "import json\n",
    "import pandas as pd\n",
    "import rdflib\n",
    "import os\n",
    "import tabulate\n",
    "import typing\n",
    "\n",
    "# mount drive here to read files from the folder \"My Drive > Colab_Notebooks > Formal_Ontology_of_Mathematics > creativity\"\n",
    "google.colab.drive.mount('/content/drive')\n",
    "\n",
    "os.chdir(\"/content/drive/My Drive/Colab_Notebooks/Formal_Ontology_of_Mathematics/creativity\")\n",
    "\n",
    "print(\"---\")\n",
    "!pwd\n",
    "\n",
    "print(\"---\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.queries as queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "file_name = \"ontology_output_v3.ttl\"\n",
    "\n",
    "# general functions\n",
    "def access_graph(file_name: str,\n",
    "                 folder_name: str = \"input\") -> rdflib.Graph:\n",
    "    input_file = os.path.join(folder_name, file_name)\n",
    "    return rdflib.Graph().parse(input_file)\n",
    "\n",
    "def sparql_to_df(kg: rdflib.Graph,\n",
    "                 sparql_query: str):\n",
    "    raw = kg.query(sparql_query)\n",
    "    variables = raw.vars\n",
    "    records = [{str(variables[i]): str(item) for i, item in enumerate(row)} for row in raw]\n",
    "    records_df = pd.DataFrame(records)\n",
    "    if \"links\" in records_df.columns:\n",
    "        records_df[\"links\"] = records_df[\"links\"].astype(int)\n",
    "    return records_df\n",
    "\n",
    "def sparql_to_concat_df(kg: rdflib.Graph,\n",
    "                        sparql_queries: list,\n",
    "                        hebb: bool = False):\n",
    "    if hebb:\n",
    "        df = pd.concat(\n",
    "            [sparql_to_df(kg, sparql_query) for sparql_query in sparql_queries],\n",
    "            ignore_index = True).groupby(by=[\"o1\", \"o2\"])[\"links\"].sum().reset_index()\n",
    "    else:\n",
    "        df = pd.concat(\n",
    "            [sparql_to_df(kg, sparql_query) for sparql_query in sparql_queries],\n",
    "            ignore_index = True).groupby(by=[\"o\"])[\"links\"].sum().reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iris_for_values(proposition_number: int):\n",
    "    iris_strings = [f\"<https://www.foom.com/core#proof_{i}> <https://www.foom.com/core#proposition_{i}>\" for i in range(1, proposition_number)]\n",
    "    return \" \".join(iris_strings)\n",
    "\n",
    "\n",
    "def history(kg: rdflib.Graph,\n",
    "            proposition_number: int = 0,\n",
    "            base_sparql_queries: list = [\n",
    "                [queries.direct_definitions(), queries.direct_postulates(), queries.direct_common_notions()],\n",
    "                [queries.hierarchical_definitions(), queries.hierarchical_postulates(), queries.hierarchical_common_notions()],\n",
    "                [queries.mereological_definitions(), queries.mereological_postulates(), queries.mereological_common_notions()] ],\n",
    "            weights: list = [6/9, 1/9, 2/9]):\n",
    "    query_sets = base_sparql_queries.copy()\n",
    "    if proposition_number >= 2:\n",
    "        # Generate the iris strings\n",
    "        iris = create_iris_for_values(proposition_number)\n",
    "\n",
    "        # Append the new queries to the existing lists\n",
    "        query_sets[0].append(queries.direct_template_propositions_proofs(iris))\n",
    "        query_sets[1].append(queries.hierarchical_template_propositions_proofs(iris))\n",
    "        query_sets[2].append(queries.mereological_template_propositions_proofs(iris))\n",
    "\n",
    "    # Generate the histories\n",
    "    histories = [sparql_to_concat_df(kg, query_set) for query_set in query_sets]\n",
    "\n",
    "    activation_dfs = []\n",
    "    # calculation of activation potentials\n",
    "    for history_df, weight in zip(histories, weights):\n",
    "        total_use = history_df[\"links\"].sum()\n",
    "        actions_df = history_df.assign(\n",
    "            activation_potential = (history_df[\"links\"] * weight) / total_use\n",
    "        )[[\"o\", \"activation_potential\"]]\n",
    "        activation_dfs.append(actions_df)\n",
    "\n",
    "    # combine dataframes\n",
    "    combined_df = pd.concat(activation_dfs, ignore_index=True)\n",
    "    return combined_df.groupby(\"o\")[\"activation_potential\"].sum().reset_index()\n",
    "\n",
    "def hebb(kg: rdflib.Graph,\n",
    "         proposition_number: int = 0,\n",
    "         sparql_queries: list = [queries.hebb_definitions(), queries.hebb_postulates(), queries.hebb_common_notions()]):\n",
    "    if proposition_number >= 2:\n",
    "        # Generate the iris strings\n",
    "        iris = create_iris_for_values(proposition_number)\n",
    "        # Append the new queries to the existing lists\n",
    "        sparql_queries.append(queries.hebb_template_propositions_proofs(iris))\n",
    "    df = sparql_to_concat_df(kg, sparql_queries, hebb=True)\n",
    "    total_use = df[\"links\"].sum()\n",
    "    df[\"activation_potential\"] = df[\"links\"] / total_use\n",
    "    df = df.drop(columns=[\"links\"])\n",
    "    df = df.sort_values(by=\"activation_potential\", ascending=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# def calculate_preface_potentials(kg: rdflib.Graph):\n",
    "def calculate_activation_potential(kg: rdflib.Graph,\n",
    "                                   proposition_number: int = 0):\n",
    "    # history potential\n",
    "    history_potential = history(kg, proposition_number)\n",
    "    print(len(history_potential))\n",
    "    print(history_potential[:20])\n",
    "    print(history_potential[\"activation_potential\"].sum())\n",
    "\n",
    "    # hebb potential\n",
    "    hebb_potential = hebb(kg, proposition_number)\n",
    "    print(len(hebb_potential))\n",
    "    print(hebb_potential[:20])\n",
    "    print(hebb_potential[\"activation_potential\"].sum())\n",
    "\n",
    "    return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. capture of last proposition: direct and mereological [TO DO]\n",
    "2. capture of history and hebbian of previous propositions and proofs [DONE]\n",
    "3. include history and hebbian of previous propositions and proofs in table [DONE]\n",
    "4. prepare table of activation potential [DONE]\n",
    "5. capture last proof: direct\n",
    "6. check last proof against lowest quartile  of activation potential table with last propostion removed\n",
    "\"\"\"\n",
    "\n",
    "def main(file_name: str = file_name,\n",
    "         proposition_number: int = 1):\n",
    "    # access turtle file and put content in the kg (rdflib.Graph)\n",
    "    kg = access_graph(file_name)\n",
    "    #  calculate activation potential\n",
    "    preface_history, preface_cooccurrence = calculate_activation_potential(kg, proposition_number)\n",
    "\n",
    "\n",
    "    # check surprise score\n",
    "\n",
    "    return\n",
    "\n",
    "main(proposition_number=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
