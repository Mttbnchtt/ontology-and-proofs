{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "print(\"---\")\n",
    "\n",
    "# install and import modules\n",
    "%pip install rdflib\n",
    "\n",
    "import google\n",
    "import pandas as pd\n",
    "import rdflib\n",
    "import os\n",
    "import typing\n",
    "\n",
    "# mount drive here to read files from the folder \"My Drive > Colab_Notebooks > Formal_Ontology_of_Mathematics > creativity\"\n",
    "google.colab.drive.mount('/content/drive')\n",
    "\n",
    "os.chdir(\"/content/drive/My Drive/Colab_Notebooks/Formal_Ontology_of_Mathematics/creativity\")\n",
    "\n",
    "print(\"---\")\n",
    "!pwd\n",
    "\n",
    "print(\"---\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8zj8IMwnqvK"
   },
   "source": [
    "# ACTIVATION POTENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "file_name = \"ontology_output_v2.ttl\"\n",
    "\n",
    "\n",
    "direct_history_preface = {\n",
    "    \"direct_history_definitions.sparql\",\n",
    "    \"direct_history_postulates.sparql\",\n",
    "    \"direct_history_common_notions.sparql\"\n",
    "}\n",
    "\n",
    "hierarhical_history_preface = {\n",
    "    \"hierarchical_history_definitions.sparql\",\n",
    "    \"hierarchical_history_postulates.sparql\",\n",
    "    \"hierarchical_history_common_notions.sparql\"\n",
    "}\n",
    "\n",
    "indirect_mereological_history_medium_importance_preface = {\n",
    "    \"horizontal_history_part1_definitions.sparql\",\n",
    "    \"horizontal_history_part1_postulates.sparql\",\n",
    "    \"horizontal_history_part1_common_notions.sparql\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general functions\n",
    "\n",
    "def access_graph(file_name: str,\n",
    "                 folder_name: str = \"input\") -> rdflib.Graph:\n",
    "    \"\"\"Accesses the RDF graph from the specified file.\n",
    "\n",
    "    Args:\n",
    "        file_name: The name of the file containing the RDF graph (e.g., \"ontology_output.ttl\").\n",
    "\n",
    "    Returns:\n",
    "        An rdflib.Graph object representing the RDF graph.\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(folder_name, file_name)\n",
    "    print(input_file)\n",
    "    return rdflib.Graph().parse(input_file)\n",
    "\n",
    "\n",
    "def run_sparql_query(knowledge_graph: rdflib.Graph,\n",
    "                     sparql_query_name: str,\n",
    "                     folder_name: str = os.path.join(\"input\", \"sparql_queries\")\n",
    "                     ) -> rdflib.query.Result:\n",
    "    \"\"\"Runs a SPARQL query on the provided knowledge graph.\n",
    "\n",
    "    Args:\n",
    "        knowledge_graph: The rdflib.Graph object representing the knowledge graph.\n",
    "        sparql_query_name: The name of the SPARQL query file (e.g., \"query_6.sparql\").\n",
    "        folder_name: The folder containing the SPARQL query file. Defaults to \"input/sparql_queries\".\n",
    "\n",
    "    Returns:\n",
    "        The result of the SPARQL query as an rdflib.query.Result object.\n",
    "    \"\"\"\n",
    "    # create path to sparql query\n",
    "    query_path = os.path.join(folder_name, sparql_query_name)\n",
    "    print(query_path)\n",
    "    # with open(query_path, \"r\") as query_file:\n",
    "    #     sparql_query = query_file.read()\n",
    "    #     print(sparql_query)\n",
    "\n",
    "    # access the sparql query and run it on the knowledge graph\n",
    "    with open(query_path, \"r\") as query_file:\n",
    "        sparql_query = query_file.read()\n",
    "    return knowledge_graph.query(sparql_query)\n",
    "\n",
    "\n",
    "def get_table_with_links(knowledge_graph: rdflib.Graph,\n",
    "                         sparql_queries: set = direct_history_preface\n",
    "                         ) -> pd.DataFrame:\n",
    "    \"\"\"Retrieves a table of textual units, conceptual items, and their usage numbers.\n",
    "\n",
    "    Executes a set of SPARQL queries on the knowledge graph to extract links between\n",
    "    textual units and conceptual items, along with their usage numbers.\n",
    "\n",
    "    Args:\n",
    "        knowledge_graph: The rdflib.Graph object representing the knowledge graph.\n",
    "        sparql_queries: A list of SPARQL query names to execute. Defaults to sparql_queries_withouth_hierarchical_imports.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with columns \"textual_unit\", \"conceptual_item\", and \"use_number\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store results\n",
    "    all_results = []\n",
    "\n",
    "    # Run SPARQL queries and append results to the list\n",
    "    for sparql_query in sparql_queries:\n",
    "        sparql_results = run_sparql_query(knowledge_graph, sparql_query)\n",
    "        for result in sparql_results:\n",
    "            all_results.append([\n",
    "                # getattr(result.s, \"toPython\", lambda: result.s)(),  # Use getattr with default lambda\n",
    "                getattr(result.o, \"toPython\", lambda: result.o)(),  # Use getattr with default lambda\n",
    "                int(result.links)\n",
    "            ])\n",
    "\n",
    "    # Return the pandas DataFrame from the list of results\n",
    "    results = pd.DataFrame(all_results,\n",
    "                           columns=[\"conceptual_item\", \"use_number\"])\n",
    "    # results = pd.DataFrame(all_results,\n",
    "    #                        columns=[\"textual_unit\", \"conceptual_item\", \"use_number\"])\n",
    "\n",
    "    # Order results\n",
    "    results = results.sort_values(by=[\"use_number\"], ascending=[False])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_-dS-krn_N6"
   },
   "source": [
    "## HISTORICAL ACTIVATION POTENTIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VgTVaGSn_R8"
   },
   "source": [
    "### DIRECT USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "History of direct use of concepts up to the given proposition.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# def get_direct_history(kg: rdflib.Graph,\n",
    "#                        sparql_queries_folder: str = os.path.join(\"input\", \"sparql_queries\")):\n",
    "#     pass\n",
    "    # get_direct_history_definitions()\n",
    "    # get_direct_history_postulates()\n",
    "    # get_direct_history_common_notions()\n",
    "\n",
    "    # return direct_history\n",
    "\n",
    "def get_preface_history(kg: rdflib.Graph,\n",
    "                        direct_history_preface: set = direct_history_preface,\n",
    "                        hierarhical_history_preface: set = hierarhical_history_preface,\n",
    "                        indirect_mereological_history_medium_importance_preface: set = indirect_mereological_history_medium_importance_preface,\n",
    "                        sparql_queries_folder: str = os.path.join(\"input\", \"sparql_queries\")):\n",
    "    # preface history: definition, postulates, common axioms\n",
    "    direct_history_preface_df = get_table_with_links(kg, direct_history_preface)\n",
    "\n",
    "    # get indirect hierarchical history\n",
    "    hierachical_history_preface_df = get_table_with_links(kg, hierarhical_history_preface)\n",
    "\n",
    "    # get indirect mereological history\n",
    "    indirect_mereological_history_preface_df = get_table_with_links(kg, indirect_mereological_history_medium_importance_preface)\n",
    "\n",
    "    return [direct_history_preface_df, hierachical_history_preface_df, indirect_mereological_history_preface_df]\n",
    "\n",
    "def historical_activation_computation(historical_activation_potential: dict,\n",
    "                                      specific_history: pd.DataFrame,\n",
    "                                      weight: float):\n",
    "    total = specific_history[\"use_number\"].sum()\n",
    "    for index in specific_history.index:\n",
    "        conceptual_item = specific_history[\"conceptual_item\"][index]\n",
    "        historical_activation_potential[conceptual_item] += ( (weight * specific_history[\"use_number\"][index]) / total )\n",
    "    return historical_activation_potential\n",
    "\n",
    "def get_historical_activation_potential(history: list,\n",
    "                                        weight_direct: float = 6/9,\n",
    "                                        weight_hierarchical: float = 1/9,\n",
    "                                        weight_mereological: float = 2/9):\n",
    "    # initialize the dictionary to compute the historical activation potential\n",
    "    historical_activation_potential = {\n",
    "        conceptual_item: 0\n",
    "            for specific_history in history\n",
    "            for conceptual_item in specific_history[\"conceptual_item\"]\n",
    "    }\n",
    "    # add the use numbers from the three histories\n",
    "    for specific_history in history:\n",
    "        activation = historical_activation_computation(\n",
    "            historical_activation_potential, specific_history, weight_direct)\n",
    "\n",
    "    # concert the dictionary to a dataframe\n",
    "    historical_activation_potential = pd.DataFrame(\n",
    "            list(historical_activation_potential.items()),\n",
    "            columns=['conceptual_item', 'activation_potential']\n",
    "        )\n",
    "\n",
    "    return historical_activation_potential\n",
    "\n",
    "\n",
    "\n",
    "def get_history(kg: rdflib.Graph,\n",
    "                       sparql_queries_folder: str = os.path.join(\"input\", \"sparql_queries\"),\n",
    "                       up_to_proposition: int = 0,\n",
    "                       base: dict = {}):\n",
    "    # history = pd.DataFrame()\n",
    "    if base:\n",
    "        print(\"base\")\n",
    "        # find highest proposition P in base\n",
    "        # if P < up_to_proposition,\n",
    "        # find the history of the propositions\n",
    "        # between P (excluded) and up_to_proposition (included)\n",
    "\n",
    "        # if P = up_to_proposition,\n",
    "        # find the history of the propositions\n",
    "        # between P (excluded) and up_to_proposition (included)\n",
    "\n",
    "        # if P > up_to_proposition,\n",
    "        # remove the history for propositions > up_to_proposition\n",
    "        # return history\n",
    "    else:\n",
    "        # get direct history of definitions, postulates, and common notions\n",
    "\n",
    "        # get direct history up to the given proposition number\n",
    "        if up_to_proposition == 0:\n",
    "            print(0)\n",
    "            history = get_preface_history(kg)\n",
    "            return history\n",
    "        elif up_to_proposition > 1:\n",
    "            pass\n",
    "            # return history\n",
    "        else:\n",
    "            return ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(file_name: str):\n",
    "    # access turtle file\n",
    "    kg = access_graph(file_name)\n",
    "\n",
    "    # direct history\n",
    "    history = get_history(kg)\n",
    "\n",
    "    historical_activation_potential = get_historical_activation_potential(history)\n",
    "\n",
    "    # indirect hierarchical history\n",
    "\n",
    "    # indirect mereological history\n",
    "\n",
    "    return history, historical_activation_potential\n",
    "\n",
    "\n",
    "history, historical_activation_potential = main(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in historical_activation_potential.index:\n",
    "    print(historical_activation_potential[\"conceptual_item\"][index], historical_activation_potential[\"activation_potential\"][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = history[0]\n",
    "for index in direct.index:\n",
    "    if not \"https\" in direct[\"conceptual_item\"][index]:\n",
    "        print(direct[\"conceptual_item\"][index])\n",
    "# direct[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical = history[1]\n",
    "\n",
    "for index in hierarchical.index:\n",
    "    if not \"https\" in hierarchical[\"conceptual_item\"][index]:\n",
    "        print(hierarchical[\"conceptual_item\"][index])\n",
    "# hierarchical[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mereological = history[2]\n",
    "for index in mereological.index:\n",
    "    if not \"https\" in mereological[\"conceptual_item\"][index]:\n",
    "        print(mereological[\"conceptual_item\"][index])\n",
    "# mereological[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSl3x7-ynqrM"
   },
   "source": [
    "### INDIRECT USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal = history[2]\n",
    "horizontal[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "file_name = \"ontology_output.ttl\"\n",
    "sparql_queries_withouth_hierarchical_imports: list = [\n",
    "    \"query_6.sparql\",\n",
    "    \"query_7.sparql\",\n",
    "    \"query_8.sparql\",\n",
    "    \"query_12.sparql\",\n",
    "    \"query_14.sparql\"\n",
    "]\n",
    "\n",
    "sparql_queries_with_hierarchical_imports: list = [\n",
    "    \"query_9.sparql\",\n",
    "    \"query_10.sparql\",\n",
    "    \"query_11.sparql\",\n",
    "    \"query_13.sparql\",\n",
    "    \"query_15.sparql\"\n",
    "]\n",
    "\n",
    "def access_graph(file_name: str) -> rdflib.Graph:\n",
    "    \"\"\"Accesses the RDF graph from the specified file.\n",
    "\n",
    "    Args:\n",
    "        file_name: The name of the file containing the RDF graph (e.g., \"ontology_output.ttl\").\n",
    "\n",
    "    Returns:\n",
    "        An rdflib.Graph object representing the RDF graph.\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(\"input\", file_name)\n",
    "    return rdflib.Graph().parse(input_file)\n",
    "\n",
    "def run_sparql_query(knowledge_graph: rdflib.Graph,\n",
    "                     sparql_query_name: str,\n",
    "                     folder_name: str = os.path.join(\"input\", \"sparql_queries\")\n",
    "                     ) -> rdflib.query.Result:\n",
    "    \"\"\"Runs a SPARQL query on the provided knowledge graph.\n",
    "\n",
    "    Args:\n",
    "        knowledge_graph: The rdflib.Graph object representing the knowledge graph.\n",
    "        sparql_query_name: The name of the SPARQL query file (e.g., \"query_6.sparql\").\n",
    "        folder_name: The folder containing the SPARQL query file. Defaults to \"input/sparql_queries\".\n",
    "\n",
    "    Returns:\n",
    "        The result of the SPARQL query as an rdflib.query.Result object.\n",
    "    \"\"\"\n",
    "    # create path to sparql query\n",
    "    query_path = os.path.join(folder_name, sparql_query_name)\n",
    "\n",
    "    # access the sparql query and run it on the knowledge graph\n",
    "    with open(query_path, \"r\") as query_file:\n",
    "        sparql_query = query_file.read()\n",
    "    return knowledge_graph.query(sparql_query)\n",
    "\n",
    "def get_table_with_links(knowledge_graph: rdflib.Graph,\n",
    "                         sparql_queries: set = sparql_queries_withouth_hierarchical_imports\n",
    "                         ) -> pd.DataFrame:\n",
    "    \"\"\"Retrieves a table of textual units, conceptual items, and their usage numbers.\n",
    "\n",
    "    Executes a set of SPARQL queries on the knowledge graph to extract links between\n",
    "    textual units and conceptual items, along with their usage numbers.\n",
    "\n",
    "    Args:\n",
    "        knowledge_graph: The rdflib.Graph object representing the knowledge graph.\n",
    "        sparql_queries: A list of SPARQL query names to execute. Defaults to sparql_queries_withouth_hierarchical_imports.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with columns \"textual_unit\", \"conceptual_item\", and \"use_number\".\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store results\n",
    "    all_results = []\n",
    "\n",
    "    # Run SPARQL queries and append results to the list\n",
    "    for sparql_query in sparql_queries:\n",
    "        sparql_results = run_sparql_query(knowledge_graph, sparql_query)\n",
    "        for result in sparql_results:\n",
    "            all_results.append([\n",
    "                getattr(result.s, \"toPython\", lambda: result.s)(),  # Use getattr with default lambda\n",
    "                getattr(result.o, \"toPython\", lambda: result.o)(),  # Use getattr with default lambda\n",
    "                int(result.links)\n",
    "            ])\n",
    "\n",
    "    # Return the pandas DataFrame from the list of results\n",
    "    return pd.DataFrame(all_results,\n",
    "                         columns=[\"textual_unit\", \"conceptual_item\", \"use_number\"])\n",
    "\n",
    "\n",
    "def dataframe_to_csv(df: pd.DataFrame,\n",
    "                     filename: str = \"output.csv\",\n",
    "                     output_folder: str = \"output\"):\n",
    "    \"\"\"Saves a pandas DataFrame to a CSV file in the 'output' folder.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to save.\n",
    "        filename: The name of the CSV file (default: \"output.csv\").\n",
    "    \"\"\"\n",
    "    # Construct the full file path\n",
    "    filepath = os.path.join(output_folder, f\"{filename}.csv\")\n",
    "\n",
    "    # Save the DataFrame to the CSV file\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"DataFrame saved to: {filepath}\")\n",
    "\n",
    "def get_tables_of_textual_units_and_concepts(file_name: str):\n",
    "    \"\"\"Retrieves tables of textual units linked to direct and indirect concepts.\n",
    "\n",
    "    This function reads an RDF graph from a file, extracts links between textual units\n",
    "    and concepts (both direct and indirect), and returns two pandas DataFrames:\n",
    "    one for direct links and one for indirect links.\n",
    "\n",
    "    Args:\n",
    "        file_name: The name of the file containing the RDF graph (e.g., \"ontology_output.ttl\").\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing two pandas DataFrames:\n",
    "            - direct_links: DataFrame with columns \"textual_unit\", \"conceptual_item\", and \"use_number\" for direct links.\n",
    "            - indirect_links: DataFrame with columns \"textual_unit\", \"conceptual_item\", and \"use_number\" for indirect links.\n",
    "    \"\"\"\n",
    "    # access turtle file\n",
    "    # and populate a Graph object\n",
    "    kg = access_graph(file_name)\n",
    "\n",
    "    # table of textual units and direct concepts\n",
    "    direct_links = get_table_with_links(kg,\n",
    "                                        sparql_queries_withouth_hierarchical_imports)\n",
    "\n",
    "    # table of textual units and indirect concepts\n",
    "    indirect_links = get_table_with_links(kg,\n",
    "                                          sparql_queries_with_hierarchical_imports)\n",
    "\n",
    "    return direct_links, indirect_links\n",
    "\n",
    "def main_get_tables_of_textual_units_and_concepts(file_name: str):\n",
    "    \"\"\"Retrieves and saves tables of textual units linked to direct and indirect concepts.\n",
    "\n",
    "    This function calls `get_tables_of_textual_units_and_concepts` to retrieve the\n",
    "    DataFrames for direct and indirect links, saves them as CSV files, and then\n",
    "    returns the DataFrames.\n",
    "\n",
    "    Args:\n",
    "        file_name: The name of the file containing the RDF graph (e.g., \"ontology_output.ttl\").\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing two pandas DataFrames:\n",
    "            - direct_links: DataFrame with columns \"textual_unit\", \"conceptual_item\", and \"use_number\" for direct links.\n",
    "            - indirect_links: DataFrame with columns \"textual_unit\", \"conceptual_item\", and \"use_number\" for indirect links.\n",
    "    \"\"\"\n",
    "    direct_links, indirect_links = get_tables_of_textual_units_and_concepts(file_name)\n",
    "\n",
    "    dataframe_to_csv(direct_links, filename = \"direct_links\")\n",
    "    dataframe_to_csv(indirect_links, filename = \"indirect_links\")\n",
    "\n",
    "    return direct_links, indirect_links\n",
    "\n",
    "direct_links, indirect_links = main_get_tables_of_textual_units_and_concepts(file_name)\n",
    "direct_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "direct_links['use_number'].plot(kind='line', figsize=(8, 4), title='use_number')\n",
    "plt.gca().spines[['top', 'right']].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "direct_links['use_number'].plot(kind='hist', bins=20, title='use_number')\n",
    "plt.gca().spines[['top', 'right',]].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indirect_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indirect_links['use_number'].plot(kind='line', figsize=(8, 4), title='use_number')\n",
    "plt.gca().spines[['top', 'right']].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "indirect_links['use_number'].plot(kind='hist', bins=20, title='use_number')\n",
    "plt.gca().spines[['top', 'right',]].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "file_name = \"ontology_output.ttl\"\n",
    "\n",
    "def access_graph(file_name: str):\n",
    "    input_file = os.path.join(\"input\", file_name)\n",
    "    return rdflib.Graph().parse(input_file)\n",
    "\n",
    "def run_sparql_query(knowledge_graph: rdflib.Graph,\n",
    "                     sparql_query_name: str,\n",
    "                     folder_name: str = os.path.join(\"input\", \"sparql_queries\")\n",
    "                     ):\n",
    "    # create path to sparql query\n",
    "    query_path = os.path.join(folder_name, sparql_queries[sparql_query_name])\n",
    "    # access the sparql query and run it on the knowledge graph\n",
    "    with open(query_path, \"r\") as query_file:\n",
    "        sparql_query = query_file.read()\n",
    "    return knowledge_graph.query(sparql_query)\n",
    "\n",
    "def get_initial_activation_potential(knowledge_graph: rdflib.Graph,\n",
    "                                     sparql_queries_direct_link: set = {\n",
    "                                         \"query_6.sparql\",\n",
    "                                         \"query_7.sparql\",\n",
    "                                         \"query_8.sparql\n",
    "                                     },\n",
    "                                     sparql_queries_hierachical_link: set = {\n",
    "                                         \"query_9.sparql\",\n",
    "                                         \"query_10.sparql\",\n",
    "                                         \"query_11.sparql\n",
    "                                     }\n",
    "                                     ):\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def prepare_initial_hebbian_connections():\n",
    "    return\n",
    "\n",
    "\n",
    "def main_routine_starting_state(file_name: str):\n",
    "    # access turtle file\n",
    "    # and populate a Graph object\n",
    "    kg = access_graph(file_name)\n",
    "\n",
    "    # run sparql queries;\n",
    "    # organize the query results;\n",
    "    # return several analytical results,\n",
    "    # including the initial state concerning the activation potential\n",
    "    # of the conceptual items in the graph\n",
    "\n",
    "    # initial_state_intermediate_results, initial_state = get_initial_state_euclid(kg)\n",
    "\n",
    "    return initial_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = \"creativity_graph.nt\"\n",
    "file_name = \"ontology_output.ttl\"\n",
    "sparql_queries: dict = {\n",
    "    \"definitions\": \"query_3.sparql\",\n",
    "    \"postulates\": \"query_1.sparql\",\n",
    "    \"common_notions\": \"query_2.sparql\",\n",
    "    \"propositions\": \"query_4.sparql\",\n",
    "    \"proofs\": \"query_5.sparql\",\n",
    "}\n",
    "\n",
    "\n",
    "def access_graph(file_name: str):\n",
    "    input_file = os.path.join(\"input\", file_name)\n",
    "    return rdflib.Graph().parse(input_file)\n",
    "\n",
    "def run_sparql_query(knowledge_graph: rdflib.Graph,\n",
    "                     sparql_query_name: str,\n",
    "                     folder_name: str = \"input\"):\n",
    "    # create path to sparql query\n",
    "    query_path = os.path.join(folder_name, sparql_queries[sparql_query_name])\n",
    "    # access the sparql query and run it on the knowledge graph\n",
    "    with open(query_path, \"r\") as query_file:\n",
    "        sparql_query = query_file.read()\n",
    "    return knowledge_graph.query(sparql_query)\n",
    "\n",
    "def get_connection_weights(knowledge_graph: rdflib.Graph,\n",
    "                           sparql_queries_initial_state: set = {\"definitions\", \"postulates\", \"common_notions\"}):\n",
    "    # initialize dictionary to store the query results\n",
    "    results: dict = {\n",
    "        \"subject\": [],\n",
    "        \"object\": [],\n",
    "        \"connection_weight\": []\n",
    "        }\n",
    "    for sparql_query_name in sparql_queries_initial_state:\n",
    "        query_results = run_sparql_query(knowledge_graph, sparql_query_name)\n",
    "        for result in query_results:\n",
    "            results[\"subject\"].append(result.s)\n",
    "            results[\"object\"].append(result.o)\n",
    "            results[\"connection_weight\"].append(int(result.links))\n",
    "\n",
    "    return results\n",
    "\n",
    "def join_results(list_of_results: list):\n",
    "    joined_results: dict = {}\n",
    "\n",
    "    for result in list_of_results:\n",
    "        for subject_triple, object_triple, connection_weight in zip(result[\"subject\"], result[\"object\"], result[\"connection_weight\"]):\n",
    "            key: str = f\"{subject_triple}||{object_triple}\"\n",
    "            joined_results[key] = joined_results.get(key, 0) + connection_weight\n",
    "    return joined_results\n",
    "\n",
    "def prepare_intermediate_results_dataframe(joined_results: dict):\n",
    "    # initial_state = pd.DataFrame(columns=[\"subject\", \"object\", \"connection_weight\"])\n",
    "    initial_state_intermediate_results: dict = {\n",
    "        \"subject\": [],\n",
    "        \"object\": [],\n",
    "        \"use_iterations\": []\n",
    "    }\n",
    "    for key, value in joined_results.items():\n",
    "        subject_triple, object_triple = key.split(\"||\")\n",
    "        initial_state_intermediate_results[\"subject\"].append(subject_triple)\n",
    "        initial_state_intermediate_results[\"object\"].append(object_triple)\n",
    "        initial_state_intermediate_results[\"use_iterations\"].append(value)\n",
    "    return pd.DataFrame(initial_state_intermediate_results)\n",
    "\n",
    "def prepare_initial_state_dataframe(initial_state_intermediate_result: pd.DataFrame):\n",
    "    # Group by the 'object' column, summing up the connection_weight\n",
    "    intial_state: pd.DataFrame = initial_state_intermediate_result.groupby('object', as_index=False)['use_iterations'].sum()\n",
    "\n",
    "    # Rename columns\n",
    "    intial_state.columns = ['conceptual_item', 'use_weight']\n",
    "\n",
    "    # Sort the DataFrame by 'use_weight' in descending order\n",
    "    intial_state = intial_state.sort_values(by='use_weight', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return intial_state\n",
    "\n",
    "\n",
    "def get_initial_state_euclid(knowledge_graph: rdflib.Graph):\n",
    "    list_of_results = []\n",
    "\n",
    "    # get connection from definitions,\n",
    "    # postulates, and common notions\n",
    "    list_of_results.append(get_connection_weights(knowledge_graph))\n",
    "\n",
    "    # join results\n",
    "    joined_results = join_results(list_of_results)\n",
    "\n",
    "    # prepare dataframe of the initial state\n",
    "    initial_state_intermediate_results_with_hierarhical_imports = prepare_intermediate_results_dataframe(joined_results)\n",
    "    initial_state_with_hierarchical_imports = prepare_initial_state_dataframe(initial_state_intermediate_results_with_hierarhical_imports)\n",
    "\n",
    "    return initial_state_intermediate_results_with_hierarhical_imports, initial_state_with_hierarchical_imports\n",
    "\n",
    "def main_routine_starting_state(file_name: str):\n",
    "    kg = access_graph(file_name)\n",
    "    initial_state_intermediate_results, initial_state = get_initial_state_euclid(kg)\n",
    "\n",
    "    return initial_state\n",
    "\n",
    "initial_state: pd.DataFrame = main_routine_starting_state(file_name)\n",
    "initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSBDR9ZqrjBc"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_initial_state_euclid(access_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
