{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "print(\"---\")\n",
    "\n",
    "# install and import modules\n",
    "%pip install rdflib\n",
    "\n",
    "import google\n",
    "import IPython\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import rdflib\n",
    "import tabulate\n",
    "import datetime\n",
    "import typing\n",
    "\n",
    "# mount drive here to read files from the folder \"My Drive > Colab_Notebooks > Formal_Ontology_of_Mathematics > creativity\"\n",
    "google.colab.drive.mount('/content/drive')\n",
    "\n",
    "os.chdir(\"/content/drive/My Drive/Colab_Notebooks/Formal_Ontology_of_Mathematics/creativity\")\n",
    "\n",
    "print(\"---\")\n",
    "!pwd\n",
    "\n",
    "print(\"---\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.queries as queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "file_name = \"ontology_output_v3.ttl\"\n",
    "\n",
    "# general functions\n",
    "def access_graph(file_name: str,\n",
    "                 folder_name: str = \"input\") -> rdflib.Graph:\n",
    "    input_file = os.path.join(folder_name, file_name)\n",
    "    return rdflib.Graph().parse(input_file)\n",
    "\n",
    "def sparql_to_df(kg: rdflib.Graph,\n",
    "                 sparql_query: str):\n",
    "    raw = kg.query(sparql_query)\n",
    "    variables = raw.vars\n",
    "    records = [{str(variables[i]): str(item) for i, item in enumerate(row)} for row in raw]\n",
    "    records_df = pd.DataFrame(records)\n",
    "    if \"links\" in records_df.columns:\n",
    "        records_df[\"links\"] = records_df[\"links\"].astype(int)\n",
    "    return records_df\n",
    "\n",
    "def sparql_to_concat_df(kg: rdflib.Graph,\n",
    "                        sparql_queries: list,\n",
    "                        hebb: bool = False):\n",
    "    if hebb:\n",
    "        df = pd.concat(\n",
    "            [sparql_to_df(kg, sparql_query) for sparql_query in sparql_queries],\n",
    "            ignore_index = True).groupby(by=[\"o1\", \"o2\"])[\"links\"].sum().reset_index()\n",
    "    else:\n",
    "        df = pd.concat(\n",
    "            [sparql_to_df(kg, sparql_query) for sparql_query in sparql_queries],\n",
    "            ignore_index = True).groupby(by=[\"o\"])[\"links\"].sum().reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iris_for_values(proposition_number: int):\n",
    "    iris_strings = [f\"<https://www.foom.com/core#proof_{i}> <https://www.foom.com/core#proposition_{i}>\" for i in range(1, proposition_number)]\n",
    "    return \" \".join(iris_strings)\n",
    "\n",
    "\n",
    "def history(kg: rdflib.Graph,\n",
    "            proposition_number: int = 0,\n",
    "            base_sparql_queries: list = [\n",
    "                [queries.direct_definitions(), queries.direct_postulates(), queries.direct_common_notions()],\n",
    "                [queries.hierarchical_definitions(), queries.hierarchical_postulates(), queries.hierarchical_common_notions()],\n",
    "                [queries.mereological_definitions(), queries.mereological_postulates(), queries.mereological_common_notions()] ],\n",
    "            weights: list = [6/9, 1/9, 2/9]):\n",
    "    query_lists = base_sparql_queries.copy()\n",
    "    if proposition_number >= 2:\n",
    "        # Generate the iris strings\n",
    "        iris = create_iris_for_values(proposition_number)\n",
    "\n",
    "        # Append the new queries to the existing lists\n",
    "        query_lists[0].append(queries.direct_template_propositions_proofs(iris))\n",
    "        query_lists[1].append(queries.hierarchical_template_propositions_proofs(iris))\n",
    "        query_lists[2].append(queries.mereological_template_propositions_proofs(iris))\n",
    "\n",
    "    # Generate the histories\n",
    "    histories = [sparql_to_concat_df(kg, query_list) for query_list in query_lists]\n",
    "\n",
    "    activation_dfs = []\n",
    "    # calculation of activation potentials\n",
    "    for history_df, weight in zip(histories, weights):\n",
    "        total_use = history_df[\"links\"].sum()\n",
    "        actions_df = history_df.assign(\n",
    "            activation_potential = (history_df[\"links\"] * weight) / total_use\n",
    "        )[[\"o\", \"activation_potential\"]]\n",
    "        activation_dfs.append(actions_df)\n",
    "\n",
    "    # combine dataframes\n",
    "    combined_df = pd.concat(activation_dfs, ignore_index=True)\n",
    "    return combined_df.groupby(\"o\")[\"activation_potential\"].sum().reset_index()\n",
    "\n",
    "def hebb(kg: rdflib.Graph,\n",
    "         proposition_number: int = 0,\n",
    "         sparql_queries: list = [queries.hebb_definitions(), queries.hebb_postulates(), queries.hebb_common_notions()]):\n",
    "    if proposition_number >= 2:\n",
    "        # Generate the iris strings\n",
    "        iris = create_iris_for_values(proposition_number)\n",
    "        # Append the new queries to the existing lists\n",
    "        sparql_queries.append(queries.hebb_template_propositions_proofs(iris))\n",
    "    df = sparql_to_concat_df(kg, sparql_queries, hebb=True)\n",
    "    total_use = df[\"links\"].sum()\n",
    "    df[\"activation_potential\"] = df[\"links\"] / total_use\n",
    "    df = df.drop(columns=[\"links\"])\n",
    "    df = df.sort_values(by=\"activation_potential\", ascending=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def calculate_activation_potential(kg: rdflib.Graph,\n",
    "                                   proposition_number: int = 0):\n",
    "    # history potential\n",
    "    calculated_history_potential_df = history(kg, proposition_number)\n",
    "    # print(len(calculated_history_potential_df))\n",
    "    # print(calculated_history_potential_df[\"activation_potential\"].sum())\n",
    "    # hebb potential\n",
    "    hebb_potential = hebb(kg, proposition_number)\n",
    "    # print(len(hebb_potential))\n",
    "    # print(hebb_potential[\"activation_potential\"].sum())\n",
    "    return calculated_history_potential_df, hebb_potential\n",
    "\n",
    "def direct_last_item(kg: rdflib.Graph,\n",
    "                     last_proposition_iri: rdflib.URIRef):\n",
    "    results = kg.query(queries.direct_template_propositions_proofs(last_proposition_iri))\n",
    "    return {str(row.o) for row in results}\n",
    "\n",
    "def mereological_last_item(kg: rdflib.Graph,\n",
    "                           last_proposition_iri: rdflib.URIRef):\n",
    "    results = kg.query(queries.direct_template_last_item(last_proposition_iri))\n",
    "    return {str(row.o) for row in results}\n",
    "\n",
    "def direct_and_mereological_last_item(kg: rdflib.Graph,\n",
    "                                      last_proposition_iri: rdflib.URIRef):\n",
    "    return direct_last_item(kg, last_proposition_iri), mereological_last_item(kg, last_proposition_iri)\n",
    "\n",
    "def highest_potential(df: pd.DataFrame,\n",
    "                      upper_part: float = 1/4):\n",
    "    keep_count = math.ceil(len(df) * upper_part)\n",
    "    print(\"keep \", keep_count)\n",
    "    return df.iloc[:keep_count].copy()\n",
    "\n",
    "def get_background_concepts(materials: dict, upper_part: float=1/4):\n",
    "    history_highest_concepts = set(highest_potential(materials[\"history\"], upper_part)[\"o\"])\n",
    "    cooccurrence_df = highest_potential(materials[\"cooccurrence\"], upper_part)\n",
    "    cooccurrence_concepts = set(cooccurrence_df[\"o1\"]) | set(cooccurrence_df[\"o2\"])\n",
    "    proposition_concepts = materials[\"direct_last_proposition\"] | materials[\"mereological_last_proposition\"]\n",
    "    return history_highest_concepts | cooccurrence_concepts | proposition_concepts\n",
    "\n",
    "def check_surprise_score(materials: dict, upper_part: float=1/4):\n",
    "    cooccurrence_df = highest_potential(materials[\"cooccurrence\"], upper_part)\n",
    "    background_concepts = get_background_concepts(materials, upper_part)\n",
    "    proof_concepts = materials[\"direct_last_proof\"]\n",
    "    print(\"background\", len(background_concepts), background_concepts)\n",
    "    diff = proof_concepts - background_concepts\n",
    "    print(\"diff \", len(diff), diff)\n",
    "    return background_concepts, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. capture of last proposition: direct and mereological [DONE]\n",
    "2. capture of history and hebbian of previous propositions and proofs [DONE]\n",
    "3. include history and hebbian of previous propositions and proofs in table [DONE]\n",
    "4. prepare table of activation potential [DONE]\n",
    "5. capture last proof: direct [DONE]\n",
    "6. check last proof against lowest 3/4  of activation potential tables with last propostion removed\n",
    "    - extract lowest 3/4 of activation potentual tables\n",
    "    - collect low activation concepts and concepts from the last proposition\n",
    "    - compare concepts above with concepts from proof. [DONE]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "CHECK UPPER_PART VALUE: it seems to do the opposite of what it should do\n",
    "\"\"\"\n",
    "\n",
    "def proof_analysis(kg: rdflib.Graph,\n",
    "                   proposition_number: int = 1,\n",
    "                   upper_part: float = 1/4):\n",
    "    #  calculate activation potential\n",
    "    history_df, cooccurrence_df = calculate_activation_potential(kg, proposition_number)\n",
    "    # find direct and mereological concepts of last proposition\n",
    "    last_proposition_iri = f\"<https://www.foom.com/core#proposition_{proposition_number}>\"\n",
    "    direct_last_proposition, mereological_last_proposition = direct_and_mereological_last_item(kg, last_proposition_iri)\n",
    "    # find direct concepts of last proof\n",
    "    last_proof_iri = f\"<https://www.foom.com/core#proof_{proposition_number}>\"\n",
    "    direct_last_proof, mereological_last_proof = direct_and_mereological_last_item(kg, last_proof_iri)\n",
    "    # check surprise score\n",
    "    materials = {\n",
    "        \"direct_last_proposition\": direct_last_proposition,\n",
    "        \"mereological_last_proposition\": mereological_last_proposition,\n",
    "        \"direct_last_proof\": direct_last_proof,\n",
    "        \"history\": history_df,\n",
    "        \"cooccurrence\": cooccurrence_df\n",
    "    }\n",
    "    # check surprisingness\n",
    "    background_concepts, diff = check_surprise_score(materials, upper_part)\n",
    "    return background_concepts, diff\n",
    "\n",
    "def output_df(analyses: list, filename: str=\"output/analyses\"):\n",
    "    analyses_df = pd.DataFrame(analyses, columns=[\"proof_number\", \"background_concepts\", \"diff\"])\n",
    "    t = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{filename}_{t}.csv\"\n",
    "    analyses_df.to_csv(filename, index=False)\n",
    "    print(f\"Output: {filename}\")\n",
    "    return analyses_df\n",
    "\n",
    "def main(file_name: str = file_name,\n",
    "         upper_proposition_number: int = 1,\n",
    "         given_upper_part: float = 1/4):\n",
    "    # access turtle file and put content in the kg (rdflib.Graph)\n",
    "    kg = access_graph(file_name)\n",
    "    # initialize list of dataframes\n",
    "    analyses = []\n",
    "    # analysis of proofs\n",
    "    for i in range(1, upper_proposition_number):\n",
    "        print(i)\n",
    "        background_concepts, diff = proof_analysis(kg, proposition_number=i, upper_part=given_upper_part)\n",
    "        print(\"----\")\n",
    "        analyses.append( [i,\n",
    "                          \" ; \".join( sorted( list(background_concepts) ) ) ,\n",
    "                          \" ; \".join( sorted(list(diff) ) ) ] )\n",
    "        print(analyses)\n",
    "    # output analyses\n",
    "    analyses_df = output_df(analyses)\n",
    "    return analyses_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses_df = main(upper_proposition_number=49, given_upper_part=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df, cooccurrence_df, direct_last_proposition, mereological_last_proposition, direct_last_proof = main(proposition_number = 1, upper_part = 1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurrence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_last_proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mereological_last_proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_last_proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analysis_to_df(proposition_number: int=1, background_concepts:set=set(), diff:set=set()):\n",
    "    return pd.DataFrame({\n",
    "        \"proposition_number\": proposition_number,\n",
    "        \"background_concepts\": \" ; \".join(sorted(list(background_concepts))),\n",
    "        \"diff\": \" ; \".join(sorted(list(diff)))\n",
    "    }, index=[0])\n",
    "\n",
    "def dfs_to_excel(dfs: list,\n",
    "                 output_file_name: str=\"output/analysis\"):\n",
    "    t = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file_name = f\"{output_file_name}_{t}.xlsx\"\n",
    "    with pd.ExcelWriter(output_file_name, engine=\"openpyxl\") as writer:\n",
    "        for i, df in enumerate(dfs):\n",
    "            df.to_excel(writer, sheet_name=f\"proof_{i+1}\", index=False)\n",
    "    print(f\"Output: {output_file_name}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
